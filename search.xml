<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[为什么阿里巴巴要求谨慎使用ArrayList中的subList方法 - 掘金]]></title>
    <url>%2F2019%2F06%2F25%2Fyuque%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E8%A6%81%E6%B1%82%E8%B0%A8%E6%85%8E%E4%BD%BF%E7%94%A8ArrayList%E4%B8%AD%E7%9A%84subList%E6%96%B9%E6%B3%95%20-%20%E6%8E%98%E9%87%91%2F</url>
    <content type="text"><![CDATA[GitHub 3.7k Star 的Java工程师成神之路 ，不来了解一下吗? GitHub 3.7k Star 的Java工程师成神之路 ，真的不来了解一下吗? GitHub 3.7k Star 的Java工程师成神之路 ，真的确定不来了解一下吗? 集合是Java开发日常开发中经常会使用到的。在之前的一些文章中，我们介绍过一些关于使用集合类应该注意的事项，如《为什么阿里巴巴禁止在 foreach 循环里进行元素的 remove/add 操作》、《为什么阿里巴巴建议集合初始化时，指定集合容量大小》等。 关于集合类，《阿里巴巴Java开发手册》中其实还有另外一个规定： 本文就来分析一下为什么会有如此建议？其背后的原理是什么？ subListsubList是List接口中定义的一个方法，该方法主要用于返回一个集合中的一段、可以理解为截取一个集合中的部分元素，他的返回值也是一个List。 如以下代码： 1234567891011public static void main(String[] args) &#123; List&lt;String&gt; names = new ArrayList&lt;String&gt;() &#123;&#123; add(&quot;Hollis&quot;); add(&quot;hollischuang&quot;); add(&quot;H&quot;); &#125;&#125;; List subList = names.subList(0, 1); System.out.println(subList);&#125;复制代码 以上代码输出结果为： 12[Hollis]复制代码 如果我们改动下代码，将subList的返回值强转成ArrayList试一下： 1234567891011public static void main(String[] args) &#123; List&lt;String&gt; names = new ArrayList&lt;String&gt;() &#123;&#123; add(&quot;Hollis&quot;); add(&quot;hollischuang&quot;); add(&quot;H&quot;); &#125;&#125;; ArrayList subList = names.subList(0, 1); System.out.println(subList);&#125;复制代码 以上代码将抛出异常： 12java.lang.ClassCastException: java.util.ArrayList$SubList cannot be cast to java.util.ArrayList复制代码 不只是强转成ArrayList会报错，强转成LinkedList、Vector等List的实现类同样也都会报错。 那么，为什么会发生这样的报错呢？我们接下来深入分析一下。 底层原理首先，我们看下subList方法给我们返回的List到底是个什么东西，这一点在JDK源码中注释是这样说的： Returns a view of the portion of this list between the specifiedfromIndex, inclusive, and toIndex, exclusive. 也就是说subList 返回是一个视图，那么什么叫做视图呢？ 我们看下subList的源码： 12345public List&lt;E&gt; subList(int fromIndex, int toIndex) &#123; subListRangeCheck(fromIndex, toIndex, size); return new SubList(this, 0, fromIndex, toIndex);&#125;复制代码 这个方法返回了一个SubList，这个类是ArrayList中的一个内部类。 SubList这个类中单独定义了set、get、size、add、remove等方法。 当我们调用subList方法的时候，会通过调用SubList的构造函数创建一个SubList，那么看下这个构造函数做了哪些事情： 123456789SubList(AbstractList&lt;E&gt; parent, int offset, int fromIndex, int toIndex) &#123; this.parent = parent; this.parentOffset = fromIndex; this.offset = offset + fromIndex; this.size = toIndex - fromIndex; this.modCount = ArrayList.this.modCount;&#125;复制代码 可以看到，这个构造函数中把原来的List以及该List中的部分属性直接赋值给自己的一些属性了。 也就是说，SubList并没有重新创建一个List，而是直接引用了原有的List（返回了父类的视图），只是指定了一下他要使用的元素的范围而已（从fromIndex（包含），到toIndex（不包含））。 所以，为什么不能讲subList方法得到的集合直接转换成ArrayList呢？因为SubList只是ArrayList的内部类，他们之间并没有集成关系，故无法直接进行强制类型转换。 视图有什么问题前面通过查看源码，我们知道，subList()方法并没有重新创建一个ArrayList，而是返回了一个ArrayList的内部类——SubList。 这个SubList是ArrayList的一个视图。 那么，这个视图又会带来什么问题呢？我们需要简单写几段代码看一下。 1、非结构性改变SubList 123456789101112131415161718192021222324public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123;&#123; add(&quot;H&quot;); add(&quot;O&quot;); add(&quot;L&quot;); add(&quot;L&quot;); add(&quot;I&quot;); add(&quot;S&quot;); &#125;&#125;; List subList = sourceList.subList(2, 5); System.out.println(&quot;sourceList ： &quot; + sourceList); System.out.println(&quot;sourceList.subList(2, 5) 得到List ：&quot;); System.out.println(&quot;subList ： &quot; + subList); subList.set(1, &quot;666&quot;); System.out.println(&quot;subList.set(3,666) 得到List ：&quot;); System.out.println(&quot;subList ： &quot; + subList); System.out.println(&quot;sourceList ： &quot; + sourceList);&#125;复制代码 得到结果： 1234567sourceList ： [H, O, L, L, I, S]sourceList.subList(2, 5) 得到List ：subList ： [L, L, I]subList.set(3,666) 得到List ：subList ： [L, 666, I]sourceList ： [H, O, L, 666, I, S]复制代码 当我们尝试通过set方法，改变subList中某个元素的值得时候，我们发现，原来的那个List中对应元素的值也发生了改变。 同理，如果我们使用同样的方法，对sourceList中的某个元素进行修改，那么subList中对应的值也会发生改变。读者可以自行尝试一下。 1、结构性改变SubList 123456789101112131415161718192021222324public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123;&#123; add(&quot;H&quot;); add(&quot;O&quot;); add(&quot;L&quot;); add(&quot;L&quot;); add(&quot;I&quot;); add(&quot;S&quot;); &#125;&#125;; List subList = sourceList.subList(2, 5); System.out.println(&quot;sourceList ： &quot; + sourceList); System.out.println(&quot;sourceList.subList(2, 5) 得到List ：&quot;); System.out.println(&quot;subList ： &quot; + subList); subList.add(&quot;666&quot;); System.out.println(&quot;subList.add(666) 得到List ：&quot;); System.out.println(&quot;subList ： &quot; + subList); System.out.println(&quot;sourceList ： &quot; + sourceList);&#125;复制代码 得到结果： 1234567sourceList ： [H, O, L, L, I, S]sourceList.subList(2, 5) 得到List ：subList ： [L, L, I]subList.add(666) 得到List ：subList ： [L, L, I, 666]sourceList ： [H, O, L, L, I, 666, S]复制代码 我们尝试对subList的结构进行改变，即向其追加元素，那么得到的结果是sourceList的结构也同样发生了改变。 1、结构性改变原List 123456789101112131415161718192021222324public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123;&#123; add(&quot;H&quot;); add(&quot;O&quot;); add(&quot;L&quot;); add(&quot;L&quot;); add(&quot;I&quot;); add(&quot;S&quot;); &#125;&#125;; List subList = sourceList.subList(2, 5); System.out.println(&quot;sourceList ： &quot; + sourceList); System.out.println(&quot;sourceList.subList(2, 5) 得到List ：&quot;); System.out.println(&quot;subList ： &quot; + subList); sourceList.add(&quot;666&quot;); System.out.println(&quot;sourceList.add(666) 得到List ：&quot;); System.out.println(&quot;sourceList ： &quot; + sourceList); System.out.println(&quot;subList ： &quot; + subList);&#125;复制代码 得到结果： 12345678910Exception in thread &quot;main&quot; java.util.ConcurrentModificationException at java.util.ArrayList$SubList.checkForComodification(ArrayList.java:1239) at java.util.ArrayList$SubList.listIterator(ArrayList.java:1099) at java.util.AbstractList.listIterator(AbstractList.java:299) at java.util.ArrayList$SubList.iterator(ArrayList.java:1095) at java.util.AbstractCollection.toString(AbstractCollection.java:454) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:131) at com.hollis.SubListTest.main(SubListTest.java:28)复制代码 我们尝试对sourceList的结构进行改变，即向其追加元素，结果发现抛出了ConcurrentModificationException。关于这个异常，我们在《一不小心就踩坑的fail-fast是个什么鬼？》中分析过，这里原理相同，就不再赘述了。 小结 我们简单总结一下，List的subList方法并没有创建一个新的List，而是使用了原List的视图，这个视图使用内部类SubList表示。 所以，我们不能把subList方法返回的List强制转换成ArrayList等类，因为他们之间没有继承关系。 另外，视图和原List的修改还需要注意几点，尤其是他们之间的相互影响： 1、对父(sourceList)子(subList)List做的非结构性修改（non-structural changes），都会影响到彼此。 2、对子List做结构性修改，操作同样会反映到父List上。 3、对父List做结构性修改，会抛出异常ConcurrentModificationException。 所以，阿里巴巴Java开发手册中有另外一条规定： ￼ 如何创建新的List如果需要对subList作出修改，又不想动原list。那么可以创建subList的一个拷贝： 123subList = Lists.newArrayList(subList);list.stream().skip(strart).limit(end).collect(Collectors.toList());复制代码 PS：最近，《阿里巴巴Java开发手册》已经正式更名为《Java开发手册》，并发布了新版本，增加了21条新规约，修改描述112处。 关注公众号后台回复：手册，即可获取最新版Java开发手册。 参考资料： www.jianshu.com/p/585485124… www.cnblogs.com/ljdblog/p/6… 哈哈]]></content>
  </entry>
  <entry>
    <title><![CDATA[30个mysql千万级大数据SQL查询优化技巧]]></title>
    <url>%2F2019%2F06%2F20%2Fyuque%2F30%E4%B8%AAmysql%E5%8D%83%E4%B8%87%E7%BA%A7%E5%A4%A7%E6%95%B0%E6%8D%AESQL%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[本文总结了30个mysql千万级大数据SQL查询优化技巧,特别适合大 数据里的MYSQL使用。 1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num=0 3.应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则引擎将放弃使用索引而进行全表扫描。 4.应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num=10 or num=20可以这样查询：select id from t where num=10 union all select id from t where num=20 5.in 和 not in 也要慎用，否则会导致全表扫描，如：select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了：select id from t where num between 1 and 3 6.下面的查询也将导致全表扫描：select id from t where name like ‘%李%’若要提高效率，可以考虑全文检索。 如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：select id from t where num=@num可以改为强制查询使用索引：select id from t with(index(索引名)) where num=@num 8.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where num/2=100应改为:select id from t where num=100*2 9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where substring(name,1,3)=’abc’ ，name以abc开头的id应改为: select id from t where name like ‘abc%’ 10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 11.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 12.不要写一些没有意义的查询，如需要生成一个空表结构：select col1,col2 into #t from t where 1=0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样： create table #t(…) 13.很多时候用 exists 代替 in 是一个好的选择：select num from a where num in(select num from b) 用下面的语句替换： select num from a where exists(select 1 from b where num=a.num) 14.并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 索引并不是越多越好，索引固然可 以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。 应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 17.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 18.尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 19.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 20.尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 21.避免频繁创建和删除临时表，以减少系统表资源的消耗。 22.临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 23.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 24.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 25.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 26.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 与临时表一样，游标并不是不可使 用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 28.在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送DONE_IN_PROC 消息。 29.尽量避免大事务操作，提高系统并发能力。 30.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 如果你的程序都能满足这30条的话那么你的程序执行效率会有很大的提高]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL 性能优化神器 Explain 使用分析]]></title>
    <url>%2F2019%2F05%2F21%2Fyuque%2FMySQL%20%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%A5%9E%E5%99%A8%20Explain%20%E4%BD%BF%E7%94%A8%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[简介MySQL 提供了一个 EXPLAIN 命令, 它可以对 SELECT 语句进行分析, 并输出 SELECT 执行的详细信息, 以供开发人员针对性优化.EXPLAIN 命令用法十分简单, 在 SELECT 语句前加上 Explain 就可以了, 例如: 1EXPLAIN SELECT * from user_info WHERE id &lt; 300; 准备为了接下来方便演示 EXPLAIN 的使用, 首先我们需要建立两个测试用的表, 并添加相应的数据: 1234567891011121314151617181920212223242526272829303132333435363738394041CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE = InnoDB DEFAULT CHARSET = utf8INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15);CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT &apos;&apos;, `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`)) ENGINE = InnoDB DEFAULT CHARSET = utf8INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出格式EXPLAIN 命令的输出内容大致如下: 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 各列的含义如下: id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type: SELECT 查询的类型. table: 查询的是哪个表 partitions: 匹配的分区 type: join 类型 possible_keys: 此次查询中可能选用的索引 key: 此次查询中确切使用到的索引. ref: 哪个字段或常数与 key 一起被使用 rows: 显示此查询一共扫描了多少行. 这个是一个估计值. filtered: 表示此查询条件所过滤的数据的百分比 extra: 额外的信息 接下来我们来重点看一下比较重要的几个字段. select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 最常见的查询类别应该是 SIMPLE 了, 比如当我们的查询没有子查询, 也没有 UNION 查询时, 那么通常就是 SIMPLE 类型, 例如: 123456789101112131415mysql&gt; explain select * from user_info where id = 2\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 如果我们使用了 UNION 查询, 那么 EXPLAIN 输出 的结果类似如下: 1234567891011mysql&gt; EXPLAIN (SELECT * FROM user_info WHERE id IN (1, 2, 3)) -&gt; UNION -&gt; (SELECT * FROM user_info WHERE id IN (3, 4, 5));+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+| 1 | PRIMARY | user_info | NULL | range | PRIMARY | PRIMARY | 8 | NULL | 3 | 100.00 | Using where || 2 | UNION | user_info | NULL | range | PRIMARY | PRIMARY | 8 | NULL | 3 | 100.00 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+3 rows in set, 1 warning (0.00 sec) table表示查询涉及的表或衍生表 typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. type 常用类型type 常用的取值有: system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可.例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的.mysql&gt; explain select * from user_info where id = 2\G _ 1. row _id: 1select_type: SIMPLEtable: user_infopartitions: NULLtype: constpossible_keys: PRIMARYkey: PRIMARYkey_len: 8ref: constrows: 1filtered: 100.00Extra: NULL1 row in set, 1 warning (0.00 sec) eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如:mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id\G _ 1. row _id: 1select_type: SIMPLEtable: order_infopartitions: NULLtype: indexpossible_keys: user_product_detail_indexkey: user_product_detail_indexkey_len: 314ref: NULLrows: 9filtered: 100.00Extra: Using where; Using index_ 2. row _id: 1select_type: SIMPLEtable: user_infopartitions: NULLtype: eq_refpossible_keys: PRIMARYkey: PRIMARYkey_len: 8ref: test.order_info.user_idrows: 1filtered: 100.00Extra: NULL2 rows in set, 1 warning (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询.例如下面这个例子中, 就使用到了 ref 类型的查询:mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5\G _ 1. row _id: 1select_type: SIMPLEtable: user_infopartitions: NULLtype: constpossible_keys: PRIMARYkey: PRIMARYkey_len: 8ref: constrows: 1filtered: 100.00Extra: NULL_ 2. row _id: 1select_type: SIMPLEtable: order_infopartitions: NULLtype: refpossible_keys: user_product_detail_indexkey: user_product_detail_indexkey_len: 9ref: constrows: 1filtered: 100.00Extra: Using index2 rows in set, 1 warning (0.01 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中.当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. 例如下面的例子就是一个范围查询: 1234567891011121314151617mysql&gt; EXPLAIN SELECT * -&gt; FROM user_info -&gt; WHERE id BETWEEN 2 AND 8 \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: rangepossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: NULL rows: 7 filtered: 100.00 Extra: Using where1 row in set, 1 warning (0.00 sec) index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据.index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index. 例如: 123456789101112131415mysql&gt; EXPLAIN SELECT name FROM user_info \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: indexpossible_keys: NULL key: name_index key_len: 152 ref: NULL rows: 10 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免.下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的.mysql&gt; EXPLAIN SELECT age FROM user_info WHERE age = 20 \G _ 1. row _id: 1select_type: SIMPLEtable: user_infopartitions: NULLtype: ALLpossible_keys: NULLkey: NULLkey_len: NULLref: NULLrows: 10filtered: 10.00Extra: Using where1 row in set, 1 warning (0.00 sec) type 类型的性能比较通常来说, 不同的 type 类型的性能关系如下:ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; systemALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的.而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快.后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. possible_keyspossible_keys 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key此字段是 MySQL 在当前查询时所真正使用到的索引. key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的栗子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id &lt; 3 AND product_name = &apos;p1&apos; AND productor = &apos;WHH&apos; \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: rangepossible_keys: user_product_detail_index key: user_product_detail_index key_len: 9 ref: NULL rows: 5 filtered: 11.11 Extra: Using where; Using index1 row in set, 1 warning (0.00 sec) 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 不过此查询语句 WHERE user_id &lt; 3 AND product_name = &#39;p1&#39; AND productor = &#39;WHH&#39; 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT &#39;0&#39;, 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = &apos;p1&apos; \G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: refpossible_keys: user_product_detail_index key: user_product_detail_index key_len: 161 ref: const,const rows: 2 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = &#39;p1&#39; 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 rowsrows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY product_name \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: NULL key: user_product_detail_index key_len: 253 ref: NULL rows: 9 filtered: 100.00 Extra: Using index; Using filesort1 row in set, 1 warning (0.00 sec) 我们的索引是 1KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort.如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name \G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: indexpossible_keys: NULL key: user_product_detail_index key_len: 253 ref: NULL rows: 9 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.]]></content>
  </entry>
  <entry>
    <title><![CDATA[上线之前]]></title>
    <url>%2F2019%2F04%2F18%2Fyuque%2F%E4%B8%8A%E7%BA%BF%E4%B9%8B%E5%89%8D%2F</url>
    <content type="text"><![CDATA[入参为空有没有处理 数据库查询为空有没有处理 数据库新增字段有没有提交到审核平台 SQL有没有命中索引 当有新增或是修改的配置时，是否及时同步到了阿波罗]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java动态追踪技术探究 - 美团技术团队]]></title>
    <url>%2F2019%2F03%2F25%2Fyuque%2FJava%E5%8A%A8%E6%80%81%E8%BF%BD%E8%B8%AA%E6%8A%80%E6%9C%AF%E6%8E%A2%E7%A9%B6%20-%20%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%2F</url>
    <content type="text"><![CDATA[引子在遥远的希艾斯星球爪哇国塞沃城中，两名年轻的程序员正在为一件事情苦恼，程序出问题了，一时看不出问题出在哪里，于是有了以下对话： “Debug一下吧。” “线上机器，没开Debug端口。” “看日志，看看请求值和返回值分别是什么？” “那段代码没打印日志。” “改代码，加日志，重新发布一次。” “怀疑是线程池的问题，重启会破坏现场。” 长达几十秒的沉默之后：“据说，排查问题的最高境界，就是只通过Review代码来发现问题。” 比几十秒长几十倍的沉默之后：“我轮询了那段代码一十七遍之后，终于得出一个结论。” “结论是？” “我还没到达只通过Review代码就能发现问题的至高境界。” 从JSP说起对于大多数Java程序员来说，早期的时候，都会接触到一个叫做JSP（Java Server Pages）的技术。虽然这种技术，在前后端代码分离、前后端逻辑分离、前后端组织架构分离的今天来看，已经过时了，但是其中还是有一些有意思的东西，值得拿出来说一说。 当时刚刚处于Java入门时期的我们，大多数精力似乎都放在了JSP的页面展示效果上了： “这个表格显示的行数不对” “原来是for循环写的有问题，改一下，刷新页面再试一遍” “嗯，好了，表格显示没问题了，但是，登录人的姓名没取到啊，是不是Sesstion获取有问题？” “有可能，我再改一下，一会儿再刷新试试” …… 在一遍一遍修改代码刷新浏览器页面重试的时候，我们自己也许并没有注意到一件很酷的事情：我们修改完代码，居然只是简单地刷新一遍浏览器页面，修改就生效了，整个过程并没有重启JVM。按照我们的常识，Java程序一般都是在启动时加载类文件，如果都像JSP这样修改完代码，不用重启就生效的话，那文章开头的问题就可以解决了啊：Java文件中加一段日志打印的代码，不重启就生效，既不破坏现场，又可以定位问题。忍不住试一试：修改、编译、替换class文件。额，不行，新改的代码并没有生效。那为什么偏偏JSP可以呢？让我们先来看看JSP的运行原理。 当我们打开浏览器，请求访问一个JSP文件的时候，整个过程是这样的: JSP文件处理过程 JSP文件修改过后，之所以能及时生效，是因为Web容器（Tomcat）会检查请求的JSP文件是否被更改过。如果发生过更改，那么就将JSP文件重新解析翻译成一个新的Sevlet类，并加载到JVM中。之后的请求，都会由这个新的Servet来处理。这里有个问题，根据Java的类加载机制，在同一个ClassLoader中，类是不允许重复的。为了绕开这个限制，Web容器每次都会创建一个新的ClassLoader实例，来加载新编译的Servlet类。之后的请求都会由这个新的Servlet来处理，这样就实现了新旧JSP的切换。 HTTP服务是无状态的，所以JSP的场景基本上都是一次性消费，这种通过创建新的ClassLoader来“替换”class的做法行得通，但是对于其他应用，比如Spring框架，即便这样做了，对象多数是单例，对于内存中已经创建好的对象，我们无法通过这种创建新的ClassLoader实例的方法来修改对象行为。 我就是想不重启应用加个日志打印，就这么难吗？ Java对象行为既然JSP的办法行不通，那我们来看看还有没有其他的办法。仔细想想，我们会发现，文章开头的问题本质上是动态改变内存中已存在对象的行为的问题。所以，我们得先弄清楚JVM中和对象行为有关的地方在哪里，有没有更改的可能性。 我们都知道，对象使用两种东西来描述事物：行为和属性。举个例子： 123456789101112131415161718192021public class Person&#123; private int age; private String name; public void speak(String str) &#123; System.out.println(str); &#125; public Person(int age, String name) &#123; this.age = age; this.name = name; &#125;&#125; 上面Person类中age和name是属性，speak是行为。对象是类的事例，每个对象的属性都属于对象本身，但是每个对象的行为却是公共的。举个例子，比如我们现在基于Person类创建了两个对象，personA和personB： 1234567Person personA = new Person(43, "lixunhuan");personA.speak("我是李寻欢");Person personB = new Person(23, "afei");personB.speak("我是阿飞"); personA和personB有各自的姓名和年龄，但是有共同的行为：speak。想象一下，如果我们是Java语言的设计者，我们会怎么存储对象的行为和属性呢？ “很简单，属性跟着对象走，每个对象都存一份。行为是公共的东西，抽离出来，单独放到一个地方。” “咦？抽离出公共的部分，跟代码复用好像啊。” “大道至简，很多东西本来都是殊途同归。” 也就是说，第一步我们首先得找到存储对象行为的这个公共的地方。一番搜索之后，我们发现这样一段描述： Method area is created on virtual machine startup, shared among all Java virtual machine threads and it is logically part of heap area. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors. Java的对象行为（方法、函数）是存储在方法区的。 “方法区中的数据从哪来？” “方法区中的数据是类加载时从class文件中提取出来的。” “class文件从哪来？” “从Java或者其他符合JVM规范的源代码中编译而来。” “源代码从哪来？” “废话，当然是手写！” “倒着推，手写没问题，编译没问题，至于加载……有没有办法加载一个已经加载过的类呢？如果有的话，我们就能修改字节码中目标方法所在的区域，然后重新加载这个类，这样方法区中的对象行为（方法）就被改变了，而且不改变对象的属性，也不影响已经存在对象的状态，那么就可以搞定这个问题了。可是，这岂不是违背了JVM的类加载原理？毕竟我们不想改变ClassLoader。” “少年，可以去看看java.lang.instrument.Instrumentation。” java.lang.instrument.Instrumentation看完文档之后，我们发现这么两个接口：redefineClasses和retransformClasses。一个是重新定义class，一个是修改class。这两个大同小异，看reDefineClasses的说明： This method is used to replace the definition of a class without reference to the existing class file bytes, as one might do when recompiling from source for fix-and-continue debugging. Where the existing class file bytes are to be transformed (for example in bytecode instrumentation) retransformClasses should be used. 都是替换已经存在的class文件，redefineClasses是自己提供字节码文件替换掉已存在的class文件，retransformClasses是在已存在的字节码文件上修改后再替换之。 当然，运行时直接替换类很不安全。比如新的class文件引用了一个不存在的类，或者把某个类的一个field给删除了等等，这些情况都会引发异常。所以如文档中所言，instrument存在诸多的限制： The redefinition may change method bodies, the constant pool and attributes. The redefinition must not add, remove or rename fields or methods, change the signatures of methods, or change inheritance. These restrictions maybe be lifted in future versions. The class file bytes are not checked, verified and installed until after the transformations have been applied, if the resultant bytes are in error this method will throw an exception. 我们能做的基本上也就是简单修改方法内的一些行为，这对于我们开头的问题，打印一段日志来说，已经足够了。当然，我们除了通过reTransform来打印日志，还能做很多其他非常有用的事情，这个下文会进行介绍。 那怎么得到我们需要的class文件呢？一个最简单的方法，是把修改后的Java文件重新编译一遍得到class文件，然后调用redefineClasses替换。但是对于没有（或者拿不到，或者不方便修改）源码的文件我们应该怎么办呢？其实对于JVM来说，不管是Java也好，Scala也好，任何一种符合JVM规范的语言的源代码，都可以编译成class文件。JVM的操作对象是class文件，而不是源码。所以，从这种意义上来讲，我们可以说“JVM跟语言无关”。既然如此，不管有没有源码，其实我们只需要修改class文件就行了。 直接操作字节码Java是软件开发人员能读懂的语言，class字节码是JVM能读懂的语言，class字节码最终会被JVM解释成机器能读懂的语言。无论哪种语言，都是人创造的。所以，理论上（实际上也确实如此）人能读懂上述任何一种语言，既然能读懂，自然能修改。只要我们愿意，我们完全可以跳过Java编译器，直接写字节码文件，只不过这并不符合时代的发展罢了，毕竟高级语言设计之始就是为我们人类所服务，其开发效率也比机器语言高很多。 对于人类来说，字节码文件的可读性远远没有Java代码高。尽管如此，还是有一些杰出的程序员们创造出了可以用来直接编辑字节码的框架，提供接口可以让我们方便地操作字节码文件，进行注入修改类的方法，动态创造一个新的类等等操作。其中最著名的框架应该就是ASM了，cglib、Spring等框架中对于字节码的操作就建立在ASM之上。 我们都知道，Spring的AOP是基于动态代理实现的，Spring会在运行时动态创建代理类，代理类中引用被代理类，在被代理的方法执行前后进行一些神秘的操作。那么，Spring是怎么在运行时创建代理类的呢？动态代理的美妙之处，就在于我们不必手动为每个需要被代理的类写代理类代码，Spring在运行时会根据需要动态地创造出一个类，这里创造的过程并非通过字符串写Java文件，然后编译成class文件，然后加载。Spring会直接“创造”一个class文件，然后加载，创造class文件的工具，就是ASM了。 到这里，我们知道了用ASM框架直接操作class文件，在类中加一段打印日志的代码，然后调用retransformClasses就可以了。 BTrace截止到目前，我们都是停留在理论描述的层面。那么如何进行实现呢？先来看几个问题： 在我们的工程中，谁来做这个寻找字节码，修改字节码，然后reTransform的动作呢？我们并非先知，不可能知道未来有没有可能遇到文章开头的这种问题。考虑到性价比，我们也不可能在每个工程中都开发一段专门做这些修改字节码、重新加载字节码的代码。 如果JVM不在本地，在远程呢？ 如果连ASM都不会用呢？能不能更通用一些，更“傻瓜”一些。 幸运的是，因为有BTrace的存在，我们不必自己写一套这样的工具了。什么是BTrace呢？BTrace已经开源，项目描述极其简短： A safe, dynamic tracing tool for the Java platform. BTrace是基于Java语言的一个安全的、可提供动态追踪服务的工具。BTrace基于ASM、Java Attach Api、Instruments开发，为用户提供了很多注解。依靠这些注解，我们可以编写BTrace脚本（简单的Java代码）达到我们想要的效果，而不必深陷于ASM对字节码的操作中不可自拔。 看BTrace官方提供的一个简单例子：拦截所有java.io包中所有类中以read开头的方法，打印类名、方法名和参数名。当程序IO负载比较高的时候，就可以从输出的信息中看到是哪些类所引起，是不是很方便？ 123456789101112131415161718package com.sun.btrace.samples;import com.sun.btrace.annotations.*;import com.sun.btrace.AnyType;import static com.sun.btrace.BTraceUtils.*;@BTrace public class ArgArray &#123; @OnMethod( clazz="/java\\.io\\..*/", method="/read.*/" ) public static void anyRead(@ProbeClassName String pcn, @ProbeMethodName String pmn, AnyType[] args) &#123; println(pcn); println(pmn); printArray(args); &#125;&#125; 再来看另一个例子：每隔2秒打印截止到当前创建过的线程数。 12345678910111213141516171819202122232425262728293031package com.sun.btrace.samples;import com.sun.btrace.annotations.*;import static com.sun.btrace.BTraceUtils.*;import com.sun.btrace.annotations.Export; @BTrace public class ThreadCounter &#123; @Export private static long count; @OnMethod( clazz="java.lang.Thread", method="start" ) public static void onnewThread(@Self Thread t) &#123; count++; &#125; @OnTimer(2000) public static void ontimer() &#123; println(count); println(Counters.perfLong("btrace.com.sun.btrace.samples.ThreadCounter.count")); &#125;&#125; 看了上面的用法是不是有所启发？忍不住冒出来许多想法。比如查看HashMap什么时候会触发rehash，以及此时容器中有多少元素等等。 有了BTrace，文章开头的问题可以得到完美的解决。至于BTrace具体有哪些功能，脚本怎么写，这些Git上BTrace工程中有大量的说明和举例，网上介绍BTrace用法的文章更是恒河沙数，这里就不再赘述了。 我们明白了原理，又有好用的工具支持，剩下的就是发挥我们的创造力了，只需在合适的场景下合理地进行使用即可。 既然BTrace能解决上面我们提到的所有问题，那么BTrace的架构是怎样的呢？ BTrace主要有下面几个模块： BTrace脚本：利用BTrace定义的注解，我们可以很方便地根据需要进行脚本的开发。 Compiler：将BTrace脚本编译成BTrace class文件。 Client：将class文件发送到Agent。 Agent：基于Java的Attach Api，Agent可以动态附着到一个运行的JVM上，然后开启一个BTrace Server，接收client发过来的BTrace脚本；解析脚本，然后根据脚本中的规则找到要修改的类；修改字节码后，调用Java Instrument的reTransform接口，完成对对象行为的修改并使之生效。 整个BTrace的架构大致如下： BTrace工作流程 BTrace最终借Instruments实现class的替换。如上文所说，出于安全考虑，Instruments在使用上存在诸多的限制，BTrace也不例外。BTrace对JVM来说是“只读的”，因此BTrace脚本的限制如下： 不允许创建对象 不允许创建数组 不允许抛异常 不允许catch异常 不允许随意调用其他对象或者类的方法，只允许调用com.sun.btrace.BTraceUtils中提供的静态方法（一些数据处理和信息输出工具） 不允许改变类的属性 不允许有成员变量和方法，只允许存在static public void方法 不允许有内部类、嵌套类 不允许有同步方法和同步块 不允许有循环 不允许随意继承其他类（当然，java.lang.Object除外） 不允许实现接口 不允许使用assert 不允许使用Class对象 如此多的限制，其实可以理解。BTrace要做的是，虽然修改了字节码，但是除了输出需要的信息外，对整个程序的正常运行并没有影响。 ArthasBTrace脚本在使用上有一定的学习成本，如果能把一些常用的功能封装起来，对外直接提供简单的命令即可操作的话，那就再好不过了。阿里的工程师们早已想到这一点，就在去年（2018年9月份），阿里巴巴开源了自己的Java诊断工具——Arthas。Arthas提供简单的命令行操作，功能强大。究其背后的技术原理，和本文中提到的大致无二。Arthas的文档很全面，想详细了解的话可以戳这里。 本文旨在说明Java动态追踪技术的来龙去脉，掌握技术背后的原理之后，只要愿意，各位读者也可以开发出自己的“冰封王座”出来。 尾声：三生万物现在，让我们试着站在更高的地方“俯瞰”这些问题。 Java的Instruments给运行时的动态追踪留下了希望，Attach API则给运行时动态追踪提供了“出入口”，ASM则大大方便了“人类”操作Java字节码的操作。 基于Instruments和Attach API前辈们创造出了诸如JProfiler、Jvisualvm、BTrace、Arthas这样的工具。以ASM为基础发展出了cglib、动态代理，继而是应用广泛的Spring AOP。 Java是静态语言，运行时不允许改变数据结构。然而，Java 5引入Instruments，Java 6引入Attach API之后，事情开始变得不一样了。虽然存在诸多限制，然而，在前辈们的努力下，仅仅是利用预留的近似于“只读”的这一点点狭小的空间，仍然创造出了各种大放异彩的技术，极大地提高了软件开发人员定位问题的效率。 计算机应该是人类有史以来最伟大的发明之一，从电磁感应磁生电，到高低电压模拟0和1的比特，再到二进制表示出几种基本类型，再到基本类型表示出无穷的对象，最后无穷的对象组合交互模拟现实生活乃至整个宇宙。 两千五百年前，《道德经》有言：“道生一，一生二，二生三，三生万物。” 两千五百年后，计算机的发展过程也大抵如此吧。 作者简介 高扬，2017年加入美团打车，负责美团打车结算系统的开发。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java后端技术]]></title>
    <url>%2F2019%2F03%2F18%2Fyuque%2FJava%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[Tomcat相关的面试题出场的几率并不高，正式因为如此，很多人忽略了对Tomcat相关技能的掌握，下面这一篇文章最早发布在知识星球，整理了Tomcat相关的系统架构，介绍了Server、Service、Connector、Container之间的关系，各个模块的功能，可以说把这几个掌握住了，Tomcat相关的面试题你就不会有任何问题了！另外，在面试的时候你还要有意识无意识的往Tomcat这个地方引，就比如说常见的Spring MVC的执行流程，一个URL的完整调用链路，这些相关的题目你是可以再往Tomcat处理请求的这个过程去说的！掌握注Tomcat这些技能了，面试官一定会佩服你的！ 另外，知识星球已经精心整理了50+的高频面试题，并且每天保证更新！每一个知识点都会由浅入深的讲解，让你知道其一也要知道其二，有兴趣的看一下，希望对你能有帮助！ 学了本节之后你应该明白的是： Server、Service、Connector、Container四大组件之间的关系和联系，以及他们的主要功能点； Tomcat执行的整体架构，请求是如何被一步步处理的； Engine、Host、Context、Wrapper相关的概念关系； Container是如何处理请求的； Tomcat用到的相关设计模式； 一、Tomcat顶层架构俗话说，站在巨人的肩膀上看世界，一般学习的时候也是先总览一下整体，然后逐个部分个个击破，最后形成思路，了解具体细节，Tomcat的结构很复杂，但是 Tomcat 非常的模块化，找到了 Tomcat最核心的模块，问题才可以游刃而解，了解了Tomcat的整体架构对以后深入了解Tomcat来说至关重要！ 先上一张Tomcat的顶层结构图（图A），如下： Tomcat中最顶层的容器是Server，代表着整个服务器，从上图中可以看出，一个Server可以包含至少一个Service，用于具体提供服务。 Service主要包含两个部分：Connector和Container。从上图中可以看出 Tomcat 的心脏就是这两个组件，他们的作用如下： 1、Connector用于处理连接相关的事情，并提供Socket与Request和Response相关的转化; 2、Container用于封装和管理Servlet，以及具体处理Request请求； 一个Tomcat中只有一个Server，一个Server可以包含多个Service，一个Service只有一个Container，但是可以有多个Connectors，这是因为一个服务可以有多个连接，如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接,示意图如下（Engine、Host、Context下边会说到）： 多个 Connector 和一个 Container 就形成了一个 Service，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给她生命、掌握其生死大权，那就非 Server 莫属了！所以整个 Tomcat 的生命周期由 Server 控制。 另外，上述的包含关系或者说是父子关系，都可以在tomcat的conf目录下的server.xml配置文件中看出，下图是删除了注释内容之后的一个完整的server.xml配置文件（Tomcat版本为8.0） 详细的配置文件文件内容可以到Tomcat官网查看： http://tomcat.apache.org/tomcat-8.0-doc/index.html 上边的配置文件，还可以通过下边的一张结构图更清楚的理解： Server标签设置的端口号为8005，shutdown=”SHUTDOWN” ，表示在8005端口监听“SHUTDOWN”命令，如果接收到了就会关闭Tomcat。一个Server有一个Service，当然还可以进行配置，一个Service有多个，Service左边的内容都属于Container的，Service下边是Connector。 二、Tomcat顶层架构小结：（1）Tomcat中只有一个Server，一个Server可以有多个Service，一个Service可以有多个Connector和一个Container；（2） Server掌管着整个Tomcat的生死大权；（4）Service 是对外提供服务的；（5）Connector用于接受请求并将请求封装成Request和Response来具体处理；（6）Container用于封装和管理Servlet，以及具体处理request请求； 知道了整个Tomcat顶层的分层架构和各个组件之间的关系以及作用，对于绝大多数的开发人员来说Server和Service对我们来说确实很远，而我们开发中绝大部分进行配置的内容是属于Connector和Container的，所以接下来介绍一下Connector和Container。 三、Connector和Container的微妙关系由上述内容我们大致可以知道一个请求发送到Tomcat之后，首先经过Service然后会交给我们的Connector，Connector用于接收请求并将接收的请求封装为Request和Response来具体处理，Request和Response封装完之后再交由Container进行处理，Container处理完请求之后再返回给Connector，最后在由Connector通过Socket将处理的结果返回给客户端，这样整个请求的就处理完了！ Connector最底层使用的是Socket来进行连接的，Request和Response是按照HTTP协议来封装的，所以Connector同时需要实现TCP/IP协议和HTTP协议！ Tomcat既然处理请求，那么肯定需要先接收到这个请求，接收请求这个东西我们首先就需要看一下Connector！ 四、Connector架构分析Connector用于接受请求并将请求封装成Request和Response，然后交给Container进行处理，Container处理完之后在交给Connector返回给客户端。 因此，我们可以把Connector分为四个方面进行理解： （1）Connector如何接受请求的？ （2）如何将请求封装成Request和Response的？ （3）封装完之后的Request和Response如何交给Container进行处理的？ （4）Container处理完之后如何交给Connector并返回给客户端的？ 首先看一下Connector的结构图（图B），如下所示： Connector就是使用ProtocolHandler来处理请求的，不同的ProtocolHandler代表不同的连接类型，比如：Http11Protocol使用的是普通Socket来连接的，Http11NioProtocol使用的是NioSocket来连接的。 其中ProtocolHandler由包含了三个部件：Endpoint、Processor、Adapter。 （1）Endpoint用来处理底层Socket的网络连接，Processor用于将Endpoint接收到的Socket封装成Request，Adapter用于将Request交给Container进行具体的处理。 （2）Endpoint由于是处理底层的Socket网络连接，因此Endpoint是用来实现TCP/IP协议的，而Processor用来实现HTTP协议的，Adapter将请求适配到Servlet容器进行具体的处理。 （3）Endpoint的抽象实现AbstractEndpoint里面定义的Acceptor和AsyncTimeout两个内部类和一个Handler接口。Acceptor用于监听请求，AsyncTimeout用于检查异步Request的超时，Handler用于处理接收到的Socket，在内部调用Processor进行处理。 至此，我们应该很轻松的回答（1）（2）（3）的问题了，但是（4）还是不知道，那么我们就来看一下Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？ 五、Container架构分析Container用于封装和管理Servlet，以及具体处理Request请求，在Connector内部包含了4个子容器，结构图如下（图C）： 4个子容器的作用分别是： （1）Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine； （2）Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点； （3）Context：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF目录以及下面的web.xml文件； （4）Wrapper：每一Wrapper封装着一个Servlet； 下面找一个Tomcat的文件目录对照一下，如下图所示： Context和Host的区别是Context表示一个应用，我们的Tomcat中默认的配置下webapps下的每一个文件夹目录都是一个Context，其中ROOT目录中存放着主应用，其他目录存放着子应用，而整个webapps就是一个Host站点。 我们访问应用Context的时候，如果是ROOT下的则直接使用域名就可以访问，例如：www.ledouit.com,如果是Host（webapps）下的其他应用，则可以使用www.ledouit.com/docs进行访问，当然默认指定的根应用（ROOT）是可以进行设定的，只不过Host站点下默认的主营用是ROOT目录下的。 看到这里我们知道Container是什么，但是还是不知道Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？别急！下边就开始探讨一下Container是如何进行处理的！ 六、Container如何处理请求的Container处理请求是使用Pipeline-Valve管道来处理的！（Valve是阀门之意） Pipeline-Valve是责任链模式，责任链模式是指在一个请求处理的过程中有很多处理者依次对请求进行处理，每个处理者负责做自己相应的处理，处理完之后将处理后的请求返回，再让下一个处理着继续处理。 但是！Pipeline-Valve使用的责任链模式和普通的责任链模式有些不同！区别主要有以下两点： （1）每个Pipeline都有特定的Valve，而且是在管道的最后一个执行，这个Valve叫做BaseValve，BaseValve是不可删除的； （2）在上层容器的管道的BaseValve中会调用下层容器的管道。 我们知道Container包含四个子容器，而这四个子容器对应的BaseValve分别在：StandardEngineValve、StandardHostValve、StandardContextValve、StandardWrapperValve。 Pipeline的处理流程图如下（图D）： （1）Connector在接收到请求后会首先调用最顶层容器的Pipeline来处理，这里的最顶层容器的Pipeline就是EnginePipeline（Engine的管道）； （2）在Engine的管道中依次会执行EngineValve1、EngineValve2等等，最后会执行StandardEngineValve，在StandardEngineValve中会调用Host管道，然后再依次执行Host的HostValve1、HostValve2等，最后在执行StandardHostValve，然后再依次调用Context的管道和Wrapper的管道，最后执行到StandardWrapperValve。 （3）当执行到StandardWrapperValve的时候，会在StandardWrapperValve中创建FilterChain，并调用其doFilter方法来处理请求，这个FilterChain包含着我们配置的与请求相匹配的Filter和Servlet，其doFilter方法会依次调用所有的Filter的doFilter方法和Servlet的service方法，这样请求就得到了处理！ （4）当所有的Pipeline-Valve都执行完之后，并且处理完了具体的请求，这个时候就可以将返回的结果交给Connector了，Connector在通过Socket的方式将结果返回给客户端。 七、总结至此，我们已经对Tomcat的整体架构有了大致的了解，从图A、B、C、D可以看出来每一个组件的基本要素和作用。我们在脑海里应该有一个大概的轮廓了！如果你面试的时候，让你简单的聊一下Tomcat，上面的内容你能脱口而出吗？当你能够脱口而出的时候，这位面试官一定会对你刮目相看的！ 热门内容： 1、IDEA一定要懂的30个快捷键！ 2、Dubbo面试18问！这些你都会吗？ 3、狗屎一样的代码！快，重构我！ 4、2019年超详细的Spring Boot知识清单 5、你选择25k的996还是18k的965？ 6、并不是所有的 Github 写在简历上都加分 7、你还在 Select * 吗？ 8、杭州互联网人的冬天]]></content>
  </entry>
  <entry>
    <title><![CDATA[面试官最爱的volatile关键字 - 掘金]]></title>
    <url>%2F2019%2F03%2F15%2Fyuque%2F%E9%9D%A2%E8%AF%95%E5%AE%98%E6%9C%80%E7%88%B1%E7%9A%84volatile%E5%85%B3%E9%94%AE%E5%AD%97%20-%20%E6%8E%98%E9%87%91%2F</url>
    <content type="text"><![CDATA[在Java相关的岗位面试中，很多面试官都喜欢考察面试者对Java并发的了解程度，而以volatile关键字作为一个小的切入点，往往可以一问到底，把Java内存模型（JMM），Java并发编程的一些特性都牵扯出来，深入地话还可以考察JVM底层实现以及操作系统的相关知识。 下面我们以一次假想的面试过程，来深入了解下volitile关键字吧！ 面试官: Java并发这块了解的怎么样？说说你对volatile关键字的理解就我理解的而言，被volatile修饰的共享变量，就具有了以下两点特性： 1 . 保证了不同线程对该变量操作的内存可见性; 2 . 禁止指令重排序 面试官: 能不能详细说下什么是内存可见性，什么又是重排序呢？这个聊起来可就多了，我还是从Java内存模型说起吧。 Java虚拟机规范试图定义一种Java内存模型（JMM）,来屏蔽掉各种硬件和操作系统的内存访问差异，让Java程序在各种平台上都能达到一致的内存访问效果。简单来说，由于CPU执行指令的速度是很快的，但是内存访问的速度就慢了很多，相差的不是一个数量级，所以搞处理器的那群大佬们又在CPU里加了好几层高速缓存。 在Java内存模型里，对上述的优化又进行了一波抽象。JMM规定所有变量都是存在主存中的，类似于上面提到的普通内存，每个线程又包含自己的工作内存，方便理解就可以看成CPU上的寄存器或者高速缓存。所以线程的操作都是以工作内存为主，它们只能访问自己的工作内存，且工作前后都要把值在同步回主内存。 这么说得我自己都有些不清楚了，拿张纸画一下： 在线程执行时，首先会从主存中read变量值，再load到工作内存中的副本中，然后再传给处理器执行，执行完毕后再给工作内存中的副本赋值，随后工作内存再把值传回给主存，主存中的值才更新。 使用工作内存和主存，虽然加快的速度，但是也带来了一些问题。比如看下面一个例子： 1i = i + 1; 假设i初值为0，当只有一个线程执行它时，结果肯定得到1，当两个线程执行时，会得到结果2吗？这倒不一定了。可能存在这种情况： 123456线程1： load i from 主存 // i = 0 i + 1 // i = 1线程2： load i from主存 // 因为线程1还没将i的值写回主存，所以i还是0 i + 1 //i = 1线程1: save i to 主存线程2： save i to 主存 如果两个线程按照上面的执行流程，那么i最后的值居然是1了。如果最后的写回生效的慢，你再读取i的值，都可能是0，这就是缓存不一致问题。 下面就要提到你刚才问到的问题了，JMM主要就是围绕着如何在并发过程中如何处理原子性、可见性和有序性这3个特征来建立的，通过解决这三个问题，可以解除缓存不一致的问题。而volatile跟可见性和有序性都有关。 面试官：那你具体说说这三个特性呢？1 . 原子性(Atomicity)： Java中，对基本数据类型的读取和赋值操作是原子性操作，所谓原子性操作就是指这些操作是不可中断的，要做一定做完，要么就没有执行。 比如： 1234i = 2;j = i;i++;i = i + 1； 上面4个操作中，i=2是读取操作，必定是原子性操作，j=i你以为是原子性操作，其实吧，分为两步，一是读取i的值，然后再赋值给j,这就是2步操作了，称不上原子操作，i++和i = i + 1其实是等效的，读取i的值，加1，再写回主存，那就是3步操作了。所以上面的举例中，最后的值可能出现多种情况，就是因为满足不了原子性。 这么说来，只有简单的读取，赋值是原子操作，还只能是用数字赋值，用变量的话还多了一步读取变量值的操作。有个例外是，虚拟机规范中允许对64位数据类型(long和double)，分为2次32为的操作来处理，但是最新JDK实现还是实现了原子操作的。 JMM只实现了基本的原子性，像上面i++那样的操作，必须借助于synchronized和Lock来保证整块代码的原子性了。线程在释放锁之前，必然会把i的值刷回到主存的。 2 . 可见性(Visibility)： 说到可见性，Java就是利用volatile来提供可见性的。 当一个变量被volatile修饰时，那么对它的修改会立刻刷新到主存，当其它线程需要读取该变量时，会去内存中读取新值。而普通变量则不能保证这一点。 其实通过synchronized和Lock也能够保证可见性，线程在释放锁之前，会把共享变量值都刷回主存，但是synchronized和Lock的开销都更大。 3 . 有序性（Ordering） JMM是允许编译器和处理器对指令重排序的，但是规定了as-if-serial语义，即不管怎么重排序，程序的执行结果不能改变。比如下面的程序段： 123double pi = 3.14; //Adouble r = 1; //Bdouble s= pi * r * r;//C 上面的语句，可以按照A-&gt;B-&gt;C执行，结果为3.14,但是也可以按照B-&gt;A-&gt;C的顺序执行，因为A、B是两句独立的语句，而C则依赖于A、B，所以A、B可以重排序，但是C却不能排到A、B的前面。JMM保证了重排序不会影响到单线程的执行，但是在多线程中却容易出问题。 比如这样的代码: 1234567891011121314int a = 0;bool flag = false;public void write() &#123; a = 2; //1 flag = true; //2&#125;public void multiply() &#123; if (flag) &#123; //3 int ret = a * a;//4 &#125; &#125; 假如有两个线程执行上述代码段，线程1先执行write，随后线程2再执行multiply，最后ret的值一定是4吗？结果不一定： 如图所示，write方法里的1和2做了重排序，线程1先对flag赋值为true，随后执行到线程2，ret直接计算出结果，再到线程1，这时候a才赋值为2,很明显迟了一步。 这时候可以为flag加上volatile关键字，禁止重排序，可以确保程序的“有序性”，也可以上重量级的synchronized和Lock来保证有序性,它们能保证那一块区域里的代码都是一次性执行完毕的。 另外，JMM具备一些先天的有序性,即不需要通过任何手段就可以保证的有序性，通常称为happens-before原则。&lt;&lt;JSR-133：Java Memory Model and Thread Specification&gt;&gt;定义了如下happens-before规则： 程序顺序规则： 一个线程中的每个操作，happens-before于该线程中的任意后续操作 监视器锁规则：对一个线程的解锁，happens-before于随后对这个线程的加锁 volatile变量规则： 对一个volatile域的写，happens-before于后续对这个volatile域的读 传递性：如果A happens-before B ,且 B happens-before C, 那么 A happens-before C start()规则： 如果线程A执行操作ThreadB_start()(启动线程B) , 那么A线程的ThreadB_start()happens-before 于B中的任意操作 join()原则： 如果A执行ThreadB.join()并且成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 interrupt()原则： 对线程interrupt()方法的调用先行发生于被中断线程代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测是否有中断发生 finalize()原则：一个对象的初始化完成先行发生于它的finalize()方法的开始 第1条规则程序顺序规则是说在一个线程里，所有的操作都是按顺序的，但是在JMM里其实只要执行结果一样，是允许重排序的，这边的happens-before强调的重点也是单线程执行结果的正确性，但是无法保证多线程也是如此。 第2条规则监视器规则其实也好理解，就是在加锁之前，确定这个锁之前已经被释放了，才能继续加锁。 第3条规则，就适用到所讨论的volatile，如果一个线程先去写一个变量，另外一个线程再去读，那么写入操作一定在读操作之前。 第4条规则，就是happens-before的传递性。 后面几条就不再一一赘述了。 面试官：volatile关键字如何满足并发编程的三大特性的？那就要重提volatile变量规则： 对一个volatile域的写，happens-before于后续对这个volatile域的读。 这条再拎出来说，其实就是如果一个变量声明成是volatile的，那么当我读变量时，总是能读到它的最新值，这里最新值是指不管其它哪个线程对该变量做了写操作，都会立刻被更新到主存里，我也能从主存里读到这个刚写入的值。也就是说volatile关键字可以保证可见性以及有序性。 继续拿上面的一段代码举例： 1234567891011121314int a = 0;bool flag = false;public void write() &#123; a = 2; //1 flag = true; //2&#125;public void multiply() &#123; if (flag) &#123; //3 int ret = a * a;//4 &#125; &#125; 这段代码不仅仅受到重排序的困扰，即使1、2没有重排序。3也不会那么顺利的执行的。假设还是线程1先执行write操作，线程2再执行multiply操作，由于线程1是在工作内存里把flag赋值为1，不一定立刻写回主存，所以线程2执行时，multiply再从主存读flag值，仍然可能为false，那么括号里的语句将不会执行。 如果改成下面这样： 12345678910111213int a = 0;volatile bool flag = false;public void write() &#123; a = 2; //1 flag = true; //2&#125;public void multiply() &#123; if (flag) &#123; //3 int ret = a * a;//4 &#125;&#125; 那么线程1先执行write,线程2再执行multiply。根据happens-before原则，这个过程会满足以下3类规则： 程序顺序规则：1 happens-before 2; 3 happens-before 4; (volatile限制了指令重排序，所以1 在2 之前执行) volatile规则：2 happens-before 3 传递性规则：1 happens-before 4 从内存语义上来看 当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存 当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效，线程接下来将从主内存中读取共享变量。 面试官：volatile的两点内存语义能保证可见性和有序性，但是能保证原子性吗？首先我回答是不能保证原子性，要是说能保证，也只是对单个volatile变量的读/写具有原子性，但是对于类似volatile++这样的复合操作就无能为力了，比如下面的例子： 12345678910111213141516171819202122public class Test &#123; public volatile int inc = 0; public void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc);&#125; 按道理来说结果是10000，但是运行下很可能是个小于10000的值。有人可能会说volatile不是保证了可见性啊，一个线程对inc的修改，另外一个线程应该立刻看到啊！可是这里的操作inc++是个复合操作啊，包括读取inc的值，对其自增，然后再写回主存。 假设线程A，读取了inc的值为10，这时候被阻塞了，因为没有对变量进行修改，触发不了volatile规则。 线程B此时也读读inc的值，主存里inc的值依旧为10，做自增，然后立刻就被写回主存了，为11。 此时又轮到线程A执行，由于工作内存里保存的是10，所以继续做自增，再写回主存，11又被写了一遍。所以虽然两个线程执行了两次increase()，结果却只加了一次。 有人说，volatile不是会使缓存行无效的吗？但是这里线程A读取到线程B也进行操作之前，并没有修改inc值，所以线程B读取的时候，还是读的10。 又有人说，线程B将11写回主存，不会把线程A的缓存行设为无效吗？但是线程A的读取操作已经做过了啊，只有在做读取操作时，发现自己缓存行无效，才会去读主存的值，所以这里线程A只能继续做自增了。 综上所述，在这种复合操作的情景下，原子性的功能是维持不了了。但是volatile在上面那种设置flag值的例子里，由于对flag的读/写操作都是单步的，所以还是能保证原子性的。 要想保证原子性，只能借助于synchronized,Lock以及并发包下的atomic的原子操作类了，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。 面试官：说的还可以，那你知道volatile底层的实现机制？如果把加入volatile关键字的代码和未加入volatile关键字的代码都生成汇编代码，会发现加入volatile关键字的代码会多出一个lock前缀指令。 lock前缀指令实际相当于一个内存屏障，内存屏障提供了以下功能： 1 . 重排序时不能把后面的指令重排序到内存屏障之前的位置2 . 使得本CPU的Cache写入内存3 . 写入动作也会引起别的CPU或者别的内核无效化其Cache，相当于让新写入的值对别的线程可见。 面试官： 你在哪里会使用到volatile，举两个例子呢？ 状态量标记，就如上面对flag的标记，我重新提一下：int a = 0;volatile bool flag = false;public void write() {a = 2; //1flag = true; //2} public void multiply() {if (flag) { //3int ret = a * a;//4}} 这种对变量的读写操作，标记为volatile可以保证修改对线程立刻可见。比synchronized,Lock有一定的效率提升。 2.单例模式的实现，典型的双重检查锁定（DCL） 1234567891011121314151617class Singleton&#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 这是一种懒汉的单例模式，使用时才创建对象，而且为了避免初始化操作的指令重排序，给instance加上了volatile。 面试官： 来给我们说说几种单例模式的写法吧，还有上面这种用法，你再详细说说呢？好吧，这又是一个话题了，volatile的问题终于问完了。。。看看你掌握了没]]></content>
  </entry>
  <entry>
    <title><![CDATA[实例分析MySQL下的四种事务隔离级别 - 后端 - 掘金]]></title>
    <url>%2F2019%2F03%2F15%2Fyuque%2F%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90MySQL%E4%B8%8B%E7%9A%84%E5%9B%9B%E7%A7%8D%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%20-%20%E5%90%8E%E7%AB%AF%20-%20%E6%8E%98%E9%87%91%2F</url>
    <content type="text"><![CDATA[数据库事务有四种隔离级别： 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据。 提交读(Read Committed)：只能读取到已经提交的数据，Oracle等多数数据库默认都是该级别。 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻读。 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞。 上面这样的教科书式定义第一次接触事务隔离概念的朋友看了可能会一脸懵逼，下面我们就通过具体的实例来解释四个隔离级别。 首先我们创建一个user表： 123456CREATE TABLE user ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, PRIMARY KEY (`id`), UNIQUE `uniq_name` USING BTREE (name)) ENGINE=`InnoDB` AUTO_INCREMENT=10 DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 读未提交隔离级别我们先将事务的隔离级别设置为read uncommitted： 12345678910mysql&gt; set session transaction isolation level read uncommitted;Query OK, 0 rows affected (0.00 sec)mysql&gt; select @@session.tx_isolation;+------------------------+| @@session.tx_isolation |+------------------------+| READ-UNCOMMITTED |+------------------------+1 row in set (0.00 sec) 在下面我们开了两个终端分别用来模拟事务一和事务二，p.s: 操作一和操作二的意思是按照时间顺序来执行的。 事务1 12345mysql&gt; start transaction; # 操作1Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into user(name) values(&apos;ziwenxie&apos;); # 操作3Query OK, 1 row affected (0.05 sec) 事务2 12345678910mysql&gt; start transaction; # 操作2Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from user; # 操作4+----+----------+| id | name |+----+----------+| 10 | ziwenxie |+----+----------+1 row in set (0.00 sec) 从上面的执行结果可以和清晰的看出来，在read uncommited级别下面我们在事务一中可能会读取到事务二中没有commit的数据，这就是脏读。 读提交隔离级别通过设置隔离级别为committed可以解决上面的脏读问题。 1mysql&gt; set session transaction isolation level read committed; 事务一 12345678910111213141516171819202122232425262728293031mysql&gt; start transaction; # 操作一Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from user; # 操作三+----+----------+| id | name |+----+----------+| 10 | ziwenxie |+----+----------+1 row in set (0.00 sec)mysql&gt; select * from user; # 操作五，操作四的修改并没有影响到事务一+----+----------+| id | name |+----+----------+| 10 | ziwenxie |+----+----------+1 row in set (0.00 sec)mysql&gt; select * from user; # 操作七+----+------+| id | name |+----+------+| 10 | lisi |+----+------+1 row in set (0.00 sec)mysql&gt; commit; # 操作八Query OK, 0 rows affected (0.00 sec) 事务二 123456789mysql&gt; start transaction; # 操作二Query OK, 0 rows affected (0.00 sec)mysql&gt; update user set name=&apos;lisi&apos; where id=10; # 操作四Query OK, 1 row affected (0.06 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; commit; # 操作六Query OK, 0 rows affected (0.08 sec) 虽然脏读的问题解决了，但是注意在事务一的操作七中，事务二在操作六commit后会造成事务一在同一个transaction中两次读取到的数据不同，这就是不可重复读问题，使用第三个事务隔离级别repeatable read可以解决这个问题。 可重复读隔离级别MySQL的Innodb存储引擎默认的事务隔离级别就是可重复读隔离级别，所以我们不用进行多余的设置。 事务一 1234567891011121314151617181920mysql&gt; start tansactoin; # 操作一mysql&gt; select * from user; # 操作五+----+----------+| id | name |+----+----------+| 10 | ziwenxie |+----+----------+1 row in set (0.00 sec)mysql&gt; commit; # 操作六Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from user; # 操作七+----+------+| id | name |+----+------+| 10 | lisi |+----+------+1 row in set (0.00 sec) 事务二 1234567mysql&gt; start tansactoin; # 操作二mysql&gt; update user set name=&apos;lisi&apos; where id=10; # 操作三Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; commit; # 操作四 在事务一的操作五中我们并没有读取到事务二在操作三中的update，只有在commit之后才能读到更新后的数据。 Innodb解决了幻读么实际上RR级别是可能产生幻读，InnoDB引擎官方称中利用MVCC多版本并发控制解决了这个问题，下面我们验证一下Innodb真的解决了幻读了么？ 为了方便展示，我修改了一下上面的user表： 1234567891011121314151617mysql&gt; alter table user add salary int(11);Query OK, 0 rows affected (0.51 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; delete from user;Query OK, 1 rows affected (0.07 sec)mysql&gt; insert into user(name, salary) value(&apos;ziwenxie&apos;, 88888888);Query OK, 1 row affected (0.07 sec)mysql&gt; select * from user;+----+----------+----------+| id | name | salary |+----+----------+----------+| 10 | ziwenxie | 88888888 |+----+----------+----------+1 row in set (0.00 sec) 事务一 1234567891011121314151617181920212223242526mysql&gt; start transaction; # 操作一Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from user; # 操作三+----+----------+----------+| id | name | salary |+----+----------+----------+| 10 | ziwenxie | 88888888 |+----+----------+----------+1 row in set (0.00 sec)mysql&gt; update user set salary=&apos;4444&apos;; # 操作六，竟然影响了两行，不是说解决了幻读么？Query OK, 2 rows affected (0.00 sec)Rows matched: 2 Changed: 2 Warnings: 0mysql&gt; select * from user; # 操作七， Innodb并没有完全解决幻读+----+----------+--------+| id | name | salary |+----+----------+--------+| 10 | ziwenxie | 4444 || 11 | zhangsan | 4444 |+----+----------+--------+2 rows in set (0.00 sec)mysql&gt; commit; # 操作八Query OK, 0 rows affected (0.04 sec) 事务二 12345678mysql&gt; start transaction; # 操作二Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into user(name, salary) value(&apos;zhangsan&apos;, &apos;666666&apos;); # 操作四Query OK, 1 row affected (0.00 sec)mysql&gt; commit; # 操作五Query OK, 0 rows affected (0.04 sec) 从上面的例子可以看出，Innodb并没有如官方所说解决幻读，不过上面这样的场景中也不是很常见不用过多的担心。 串行化隔离级别所有事务串行执行，最高隔离级别，不会出现幻读性能会很差，实际开发中很少使用到。 备注 脏读：读到了其他会话还未提交的更新 幻读：读取到其他会话的新增或者删除，侧重于数量的改变 不可重复读：同一个会话中两次执行相同的语句得到的结果不同 ContactGitHub: github.com/ziwenxieBlog: www.ziwenxie.site 本文同步发于我的个人博客，转载请声明博客出处]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL使用可重复读作为默认隔离级别的原因 - vinchen - 博客园]]></title>
    <url>%2F2019%2F02%2F27%2Fyuque%2FMySQL%E4%BD%BF%E7%94%A8%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E4%BD%9C%E4%B8%BA%E9%BB%98%E8%AE%A4%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E7%9A%84%E5%8E%9F%E5%9B%A0%20-%20vinchen%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD%2F</url>
    <content type="text"><![CDATA[一般的DBMS系统，默认都会使用读提交（Read-Comitted，RC）作为默认隔离级别，如Oracle、SQL Server等，而MySQL却使用可重复读（Read-Repeatable，RR）。要知道，越高的隔离级别，能解决的数据一致性问题越多，理论上性能损耗更大，可并发性越低。隔离级别依次为 SERIALIZABLE &gt; RR &gt; RC &gt; Read-Uncommited 在SQL标准中，前三种隔离级别分别解决了幻象读、不可重复读和脏读的问题。那么，为什么MySQL使用可重复读作为默认隔离级别呢？ Binlog是MySQL的逻辑操作日志，广泛应用于复制和恢复。MySQL 5.1以前，Statement是Binlog的默认格式，即依次记录系统接受的SQL请求；5.1及以后，MySQL提供了Row和Mixed两个Binlog格式。 从MySQL 5.1开始，如果打开语句级Binlog，就不支持RC和Read-Uncommited隔离级别。要想使用RC隔离级别，必须使用Mixed或Row格式。 mysql&gt; set tx_isolation=’read-committed’; Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into t1 values(1,1); ERROR 1598 (HY000): Binary logging not possible. Message: Transaction level ‘READ-COMMITTED’ in InnoDB is not safe for binlog mode ‘STATEMENT’ 那么，为什么RC隔离级别不支持语句级Binlog呢？我们关闭binlog，做以下测试。 会话1 会话2 use test; #初始化数据 create table t1(c1 int, c2 int) engine=innodb; create table t2(c1 int, c2 int) engine=innodb; insert into t1 values(1,1), (2,2); insert into t2 values(1,1), (2,2); #设置隔离级别 set tx_isolation=’read-committed’; Query OK, 0 rows affected (0.00 sec) #连续更新两次 mysql&gt; Begin; Query OK, 0 rows affected (0.03 sec) mysql&gt; update t2 set c2 = 3 where c1 in (select c1 from t1); Query OK, 2 rows affected (0.00 sec) Rows matched: 2 Changed: 2 Warnings: 0 mysql&gt; update t2 set c2 = 4 where c1 in (select c1 from t1); Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql&gt; select * from t2; +——+——+ | c1 | c2 | +——+——+ | 1 | 4 | | 2 | 3 | +——+——+ 2 rows in set (0.00 sec) mysql&gt; commit; #设置隔离级别 set tx_isolation=’read-committed’; Query OK, 0 rows affected (0.00 sec) #两次更新之间执行删除 mysql&gt; delete from t1 where c1 = 2; Query OK, 1 row affected (0.03 sec) 由以上测试知，RC隔离级别下，会话2执行时序在会话1事务的语句之间，并且会话2的操作影响了会话1的结果，这会对Binlog结果造成影响。 由于Binlog中语句的顺序以commit为序，如果语句级Binlog允许，两会话的执行时序是 #会话2 set tx_isolation=’read-committed’; delete from t1 where c1 = 2; commit; #会话1 set tx_isolation=’read-committed’; Begin; update t2 set c2 = 3 where c1 in (select c1 from t1); update t2 set c2 = 4 where c1 in (select c1 from t1); select * from t2; +——+——+ | c1 | c2 | +——+——+ | 1 | 4 | | 2 | 2 | +——+——+ 2 rows in set (0.00 sec) commit; 由上可知，在MySQL 5.1及以上的RC隔离级别下，语句级Binlog在DR上执行的结果是不正确的！ 那么，MySQL 5.0呢？5.0允许RC下语句级Binlog，是不是说很容易产生DB/DR不一致呢？ 事实上，在5.0重复上述一个测试，并不存在这个问题，原因是5.0的RC与5.1的RR使用类似的并发和上锁机制，也就是说，MySQL 5.0的RC与5.1及以上的RC可能存在兼容性问题。 下面看看RR是怎么解决这个问题的。 导致RC隔离级别DB/DR不一致的原因是：RC不可重复读，而Binlog要求SQL串行化！ 在RR下，重复以上测试 会话1 会话2 use test; #初始化数据 create table t1(c1 int, c2 int) engine=innodb; create table t2(c1 int, c2 int) engine=innodb; insert into t1 values(1,1), (2,2); insert into t2 values(1,1), (2,2); #设置隔离级别 set tx_isolation=’repeatable-read’; Query OK, 0 rows affected (0.00 sec) #连续更新两次 mysql&gt; Begin; Query OK, 0 rows affected (0.03 sec) mysql&gt; update t2 set c2 = 3 where c1 in (select c1 from t1); Query OK, 2 rows affected (0.00 sec) Rows matched: 2 Changed: 2 Warnings: 0 mysql&gt; update t2 set c2 = 4 where c1 in (select c1 from t1); Query OK, 2 rows affected (0.00 sec) Rows matched: 2 Changed: 2 Warnings: 0 mysql&gt; select * from t2; +——+——+ | c1 | c2 | +——+——+ | 1 | 4 | | 2 | 4 | +——+——+ 2 rows in set (0.00 sec) mysql&gt; commit; #设置隔离级别 set tx_isolation=’ repeatable-read’; Query OK, 0 rows affected (0.00 sec) #两次更新之间执行删除 mysql&gt; delete from t1 where c1 = 2; –阻塞，直到会话1提交 Query OK, 1 row affected (18.94 sec) 与RC隔离级别不同的是，在RR中，由于保证可重复读，会话2的delete语句会被会话1阻塞，直到会话1提交。 在RR中，会话1语句update t2 set c2 = 3 where c1 in (select c1 from t1)会先在t1的记录上S锁（5.1的RC中不会上这个锁，但5.0的RC会），接着在t2的满足条件的记录上X锁。由于会话1没提交，会话2的delete语句需要等待会话1的S锁释放，于是阻塞。 因此，在RR中，以上测试会话1、会话2的依次执行，与Binlog的顺序一致，从而保证DB/DR一致。 幻象读除了保证可重复读，MySQL的RR还一定程度上避免了幻象读（幻象读是由于插入导致的新记录）。（为什么说一定程度呢？参考第3节可重复读和串行化的区别。） 会话1 会话2 use test; #初始化数据 create table t1(c1 int primary key, c2 int) engine=innodb; create table t2(c1 int primary key, c2 int) engine=innodb; insert into t1 values(1,1), (10,10); insert into t2 values(1,1), (5,5), (10,10); #设置隔离级别 set tx_isolation=’repeatable-read’; Query OK, 0 rows affected (0.00 sec) #连续更新两次 mysql&gt; Begin; Query OK, 0 rows affected (0.03 sec) mysql&gt; update t2 set c2 = 20 where c1 in (select c1 from t1); Query OK, 2 rows affected (0.00 sec) Rows matched: 2 Changed: 2 Warnings: 0 mysql&gt; delete from where c1 in (select c1 from t1); Query OK, 2 rows affected (0.00 sec) Rows matched: 2 Changed: 2 Warnings: 0 mysql&gt; select * from t2; +——+——+ | c1 | c2 | +——+——+ | 5 | 5 | +——+——+ 2 rows in set (0.00 sec) mysql&gt; commit; #设置隔离级别 set tx_isolation=’ repeatable-read’; Query OK, 0 rows affected (0.00 sec) #两次更新之间执行插入 mysql&gt; insert into t1 values(5,5); –阻塞，直到会话1提交 Query OK, 1 row affected (18.94 sec) 由上述例子知，会话2的插入操作被阻塞了，原因是RR隔离级别中，除了记录锁外，还会上间隙锁(gap锁)。例如，对于表t1，update t2 set c2 = 20 where c1 in (select c1 from t1)以上的锁包括： (-∞, 1), 1, (1, 10), 10, (10, +∞) 由于对t1做全表扫描，因此，所有记录和间隙都要上锁，其中(x,y)表示间隙锁，数字表示记录锁，全部都是S锁。会话2的insert操作插入5，位于间隙(1,10)，需要获得这个间隙的X锁，因此两操作互斥，会话2阻塞。 SQL标准的RR并不要求避免幻象读，而InnoDB通过gap锁来避免幻象，从而实现SQL的可串行化，保证Binlog的一致性。 要想取消gap lock，可使用参数innodb_lock_unsafe_for_binlog=1，默认为0。 InnoDB的RR可以避免不可重复读和幻象读，那么与串行化有什么区别呢？ 会话1 会话2 use test; #初始化数据 create table t3(c1 int primary key, c2 int) engine=innodb; #设置隔离级别 set tx_isolation=’repeatable-read’; Query OK, 0 rows affected (0.00 sec) mysql&gt; Begin; Query OK, 0 rows affected (0.03 sec) mysql&gt; select * from t3 where c1 = 1; Empty set (0.00 sec) mysql&gt; select * from t3 where c1 = 1; Empty set (0.00 sec) mysql&gt; update t3 set c2 =2 where c1 = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql&gt; select * from t3 where c1 = 1; +—-+——+ | c1 | c2 | +—-+——+ | 1 | 2 | +—-+——+ 1 row in set (0.00 sec) mysql&gt; commit; #设置隔离级别 set tx_isolation=’ repeatable-read’; Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into t3 values(1,1); Query OK, 1 row affected (0.05 sec) 由上述会话1中，连续两次读不到数据，但更新却成功，并且更新后的相同读操作就能读到数据了，这算不算幻读呢？ 其实，RR隔离级别的防止幻象主要是针对写操作的，即只保证写操作的可串行化，因为只有写操作影响Binlog；而读操作是通过MVCC来保证一致性读（无幻象）。 然而，可串行化隔离级别要求读写可串行化。使用可串行化重做以上测试。 会话1 会话2 use test; #初始化数据 create table t3(c1 int primary key, c2 int) engine=innodb; #设置隔离级别 set tx_isolation=’SERIALIZABLE’; Query OK, 0 rows affected (0.00 sec) mysql&gt; Begin; Query OK, 0 rows affected (0.03 sec) mysql&gt; select * from t3 where c1 = 1; Empty set (0.00 sec) mysql&gt; select * from t3 where c1 = 1; Empty set (0.00 sec) mysql&gt; update t3 set c2 =2 where c1 = 1; Query OK, 0 rows affected (0.00 sec) Rows matched: 0 Changed: 0 Warnings: 0 mysql&gt; select * from t3 where c1 = 1; Empty set (0.00 sec) mysql&gt; commit; #设置隔离级别 set tx_isolation=’SERIALIZABLE’; Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into t3 values(1,1); #阻塞，直到会话1提交 Query OK, 1 row affected (48.90 sec) 设置为串行化后，会话2的插入操作被阻塞。由于在串行化下，查询操作不在使用MVCC来保证一致读，而是使用S锁来阻塞其他写操作。因此做到读写可串行化，然而换来就是并发性能的大大降低。 MySQL使用可重复读来作为默认隔离级别的主要原因是语句级的Binlog。RR能提供SQL语句的写可串行化，保证了绝大部分情况（不安全语句除外）的DB/DR一致。 另外，通过这个测试发现MySQL 5.0与5.1在RC下表现是不一样的，可能存在兼容性问题。 http://dev.mysql.com/doc/refman/5.1/en/binary-log-mixed.html http://dev.mysql.com/doc/refman/5.1/en/set-transaction.html http://dev.mysql.com/doc/refman/5.0/en/set-transaction.html http://dev.mysql.com/doc/refman/5.5/en/innodb-parameters.html#sysvar_innodb_locks_unsafe_for_binlog http://blog.bitfly.cn/post/mysql-innodb-phantom-read/]]></content>
  </entry>
  <entry>
    <title><![CDATA[常用的sql语句]]></title>
    <url>%2F2019%2F02%2F25%2Fyuque%2F%E5%B8%B8%E7%94%A8%E7%9A%84sql%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[交换两列的值123update models_mapping as a, models_mapping as b set a.souche_model_code=b.third_model_code, a.third_model_code=b.souche_model_codewhere a.id=b.id and a.src_domain='iautos.cn' and a.domain='souche.com'; 查询表的所有字段1234567select GROUP_CONCAT(CONCAT('\`',COLUMN_NAME,'\`')) from INFORMATION_SCHEMA.Columns where table_name='brand' and table_schema='car_model' select COLUMN_NAME,column_comment from INFORMATION_SCHEMA.Columns where table_name='表名' and table_schema='数据库名' 一次新增多个字段1234alter table models_mapping add (`souche_series_name` varchar(255) DEFAULT NULL COMMENT '来源车系名称',`third_brand_name` varchar(255) DEFAULT NULL COMMENT '目的域品牌名称',`third_series_name` varchar(255) DEFAULT NULL COMMENT '目的域车系名称',`third_model_name` varchar(255) DEFAULT NULL COMMENT '目的域车型名称'); 复制表数据1insert into dictionary(field,`key`,`value`,date_create,date_update) select field,dict_key ,`value`,data_create , data_create from config_dictionary ; 新增唯一索引1alter table `dictionary` add UNIQUE uniq_field_value(`field`,`value`); 查询参数价格颜色1234567891011121314151617181920212223242526select a.model_code '车型编码',a.model_name '车型名称',a.category_code '车款编码',a.category_name '车款名称',a.series_code '车系编码',a.series_name '车系名称',a.brand_code '品牌编码',a.brand_name '品牌名称',(select Group_concat(color_name SEPARATOR ',') from model_color where model_code = a.model_code and type=0) '外饰颜色',(select Group_concat(color_name SEPARATOR ',') from model_color where model_code = a.model_code and type=1) '内饰颜色',(select guide_price from model_price where model_code = a.model_code) '指导价',(select assurance_period_month from model_parameter where model_code = a.model_code) '保养周期-月',(select assurance_period_km from model_parameter where model_code = a.model_code) '保养周期-公里'from model awhere a.model_code in ( select model_code from model where brand_code in ('brand-74','brand-86','brand-48')) 查询车型参数12345678910111213141516171819202122select a.model_code '车型编码',a.model_name '车型名称',a.category_code '车款编码',a.category_name '车款名称',a.series_code '车系编码',a.series_name '车系名称',a.brand_code '品牌编码',a.brand_name '品牌名称',(select value from dictionary where `field`='fuelForm' and `key`=b.fuel_form) '燃料形式',(select value from dictionary where `field`='engineVolume' and `key`=b.engine_volume) '排量',(select value from dictionary where `field`='gearBox' and `key`=b.gear_box) '变速',(select value from dictionary where `field`='drivingMode' and `key`=b.driving_Mode) '驱动方式',(select value from dictionary where `field`='intakeType' and `key`=b.intake_type) '进气形式',(select value from dictionary where `field`='bodyFormId' and `key`=b.body_formid) '车身形式标识'from model a, model_parameter bwhere a.model_code = b.model_codeand a.display_tag &amp; 4&gt;0; 修改列字段1alter table model_parameter modify column `max_power` decimal(6,2) unsigned DEFAULT NULL COMMENT '最大功率(kW)';]]></content>
  </entry>
  <entry>
    <title><![CDATA[批量任务体现多线程的威力！ - 掘金]]></title>
    <url>%2F2019%2F02%2F05%2Fyuque%2F%E6%89%B9%E9%87%8F%E4%BB%BB%E5%8A%A1%E4%BD%93%E7%8E%B0%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%A8%81%E5%8A%9B%EF%BC%81%20-%20%E6%8E%98%E9%87%91%2F</url>
    <content type="text"><![CDATA[对于多线程的理解不是非常深刻，工作中用到多线程代码的机会也不多，前不久遇到了一个使用场景，通过编码实现后对于多线程的理解和应用有了更加深刻的理解。场景如下：现有给用户发送产品调研的需求，运营的同事拿来了一个Excel文件，要求给Excel里面大约六万个手机号发送调研短信。 最简单的方法就是一个循环然后单线程顺序发送，但是核心问题在于，给短信运营商发短信的接口响应时间较长，假设平均100ms的响应时间，那么单线程发送的话需要6万*0.1秒=6000秒。显然这个时间是不能接受的，运营商系统的发送接口我们是不能优化的，只得增强自己的发送和处理能力才能尽快的完成任务。 读取Excel中的信息 包依赖工具类代码，Maven中引入如下两个包 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;3.17&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlbeans&lt;/groupId&gt; &lt;artifactId&gt;xmlbeans&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt;&lt;/dependency&gt;复制代码 读取Excel的工具类代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * 读取Excel的文件信息 * * @param fileName */public static void readFromExcel(String fileName) &#123; InputStream is = null; try &#123; is = new FileInputStream(fileName); XSSFWorkbook workbook = new XSSFWorkbook(is); XSSFSheet sheet = workbook.getSheetAt(0); int num = 0; // 循环行Row for (int rowNum = 0, lastNum = sheet.getLastRowNum(); rowNum &lt;= lastNum; rowNum++) &#123; XSSFRow row = sheet.getRow(rowNum); String phoneNumber = getStringValueFromCell(row.getCell(0)).trim(); phoneList.add(phoneNumber); &#125; System.out.println(num); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; /** * 读取Excel里面Cell内容 * * @param cell * @return */private static String getStringValueFromCell(XSSFCell cell) &#123; // 单元格内的时间格式 SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); // 单元格内的数字类型 DecimalFormat decimalFormat = new DecimalFormat(&quot;#.#####&quot;); // 单元格默认为空 String cellValue = &quot;&quot;; if (cell == null) &#123; return cellValue; &#125; // 按类型读取 if (cell.getCellType() == XSSFCell.CELL_TYPE_STRING) &#123; cellValue = cell.getStringCellValue(); &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_NUMERIC) &#123; // 日期转为时间形式 if (DateUtil.isCellDateFormatted(cell)) &#123; double d = cell.getNumericCellValue(); Date date = DateUtil.getJavaDate(d); cellValue = dateFormat.format(date); &#125; else &#123; // 其他转为数字 cellValue = decimalFormat.format((cell.getNumericCellValue())); &#125; &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_BLANK) &#123; cellValue = &quot;&quot;; &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_BOOLEAN) &#123; cellValue = String.valueOf(cell.getBooleanCellValue()); &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_ERROR) &#123; cellValue = &quot;&quot;; &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_FORMULA) &#123; cellValue = cell.getCellFormula().toString(); &#125; return cellValue;&#125; 复制代码 模拟运营商发送短信的方法1234567891011121314/** * 外部接口耗时长，通过多线程增强 * * @param userPhone */public void sendMsgToPhone(String userPhone) &#123; try &#123; Thread.sleep(SEND_COST_TIME); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;send message to : &quot; + userPhone);&#125;复制代码 多线程发短信 简单的单线程发送12345678910111213141516171819/** * 单线程发送 * * @param phoneList * @return */ private long singleThread(List&lt;String&gt; phoneList) &#123; long start = System.currentTimeMillis(); /*// 直接主线程执行 for (String phoneNumber : phoneList) &#123; threadOperation.sendMsgToPhone(phoneNumber); &#125;*/ SendMsgExtendThread smet = threadOperation.new SendMsgExtendThread(phoneList); smet.start(); long totalTime = System.currentTimeMillis() - start; System.out.println(&quot;单线程发送总时间：&quot; + totalTime); return totalTime; &#125;复制代码 对于大批量发短信的场景，如果使用单线程将全部一千个号码发送完毕的话，大约需要103132ms，可见效率低下，耗费时间较长。 多线程发送短信中的一个核心要点是，将全部手机号码拆分成多个组后，分配给每个线程进行执行。 两个线程的示例1234567891011121314151617/** * 两个线程发送 * * @param phoneList * @return */private long twoThreads(List&lt;String&gt; phoneList) &#123; long start = System.currentTimeMillis(); List&lt;String&gt; list1 = phoneList.subList(0, phoneList.size() / 2); List&lt;String&gt; list2 = phoneList.subList(phoneList.size() / 2, phoneList.size()); SendMsgExtendThread smet = threadOperation.new SendMsgExtendThread(list1); smet.start(); SendMsgExtendThread smet1 = threadOperation.new SendMsgExtendThread(list2); smet1.start(); return 0;&#125;复制代码 另一种数据分组方式1234567891011121314151617/** * 另外一种分配方式 * * @param phoneList */private void otherThread(List&lt;String&gt; phoneList) &#123; for (int threadNo = 0; threadNo &lt; 10; threadNo++) &#123; int numbersPerThread = 10; List&lt;String&gt; list = phoneList.subList(threadNo * numbersPerThread, (threadNo * numbersPerThread) + 10); SendMsgExtendThread smet = threadOperation.new SendMsgExtendThread(list); smet.start(); if (list.size() &lt; numbersPerThread) &#123; break; &#125; &#125;&#125;复制代码 线程池发送123456789101112131415/** * 线程池发送 * * @param phoneList * @return */private void threadPool(List&lt;String&gt; phoneList) &#123; for (int threadNo = 0; threadNo &lt; THREAD_POOL_SIZE; threadNo++) &#123; int numbersPerThread = 10; List&lt;String&gt; list = phoneList.subList(threadNo * numbersPerThread, (threadNo * numbersPerThread) + 10); threadOperation.executorService.execute(threadOperation.new SendMsgExtendThread(list)); &#125; threadOperation.executorService.shutdown();&#125;复制代码 使用Callable发送123456789101112131415161718192021222324252627/** * 多线程发送 * * @param phoneList * @return */private void multiThreadSend(List&lt;String&gt; phoneList) &#123; List&lt;Future&lt;Long&gt;&gt; futures = new ArrayList&lt;&gt;(); for (int threadNo = 0; threadNo &lt; THREAD_POOL_SIZE; threadNo++) &#123; int numbersPerThread = 100; List&lt;String&gt; list = phoneList.subList(threadNo * numbersPerThread, (threadNo * numbersPerThread) + 100); Future&lt;Long&gt; future = threadOperation.executorService.submit(threadOperation.new SendMsgImplCallable(list, String.valueOf(threadNo))); futures.add(future); &#125; for (Future&lt;Long&gt; future : futures) &#123; try &#123; System.out.println(future.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; threadOperation.executorService.shutdown();&#125;复制代码 使用多线程发送，将发送任务进行分割然后分配给每个线程执行，执行完毕需要10266ms，可见执行效率明显提升，消耗时间明显缩短。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285package com.lingyejun.tick.authenticator; import org.apache.poi.ss.usermodel.DateUtil;import org.apache.poi.xssf.usermodel.XSSFCell;import org.apache.poi.xssf.usermodel.XSSFRow;import org.apache.poi.xssf.usermodel.XSSFSheet;import org.apache.poi.xssf.usermodel.XSSFWorkbook; import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;import java.text.DecimalFormat;import java.text.SimpleDateFormat;import java.util.*;import java.util.concurrent.*; public class ThreadOperation &#123; // 发短信的同步等待时间 private static final long SEND_COST_TIME = 100L; // 手机号文件 private static final String FILE_NAME = "/Users/lingye/Downloads/phone_number.xlsx"; // 手机号列表 private static List&lt;String&gt; phoneList = new ArrayList&lt;&gt;(); // 单例对象 private static volatile ThreadOperation threadOperation; // 线程个数 private static final int THREAD_POOL_SIZE = 10; // 初始化线程池 private ExecutorService executorService = new ThreadPoolExecutor(THREAD_POOL_SIZE, THREAD_POOL_SIZE, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); public ThreadOperation() &#123; // 从本地文件中读取手机号码 readFromExcel(FILE_NAME); &#125; public static void main(String[] args) &#123; ThreadOperation threadOperation = getInstance(); //threadOperation.singleThread(phoneList); threadOperation.multiThreadSend(phoneList); &#125; /** * 单例获取对象 * * @return */ public static ThreadOperation getInstance() &#123; if (threadOperation == null) &#123; synchronized (ThreadOperation.class) &#123; if (threadOperation == null) &#123; threadOperation = new ThreadOperation(); &#125; &#125; &#125; return threadOperation; &#125; /** * 读取Excel的文件信息 * * @param fileName */ public static void readFromExcel(String fileName) &#123; InputStream is = null; try &#123; is = new FileInputStream(fileName); XSSFWorkbook workbook = new XSSFWorkbook(is); XSSFSheet sheet = workbook.getSheetAt(0); int num = 0; // 循环行Row for (int rowNum = 0, lastNum = sheet.getLastRowNum(); rowNum &lt;= lastNum; rowNum++) &#123; XSSFRow row = sheet.getRow(rowNum); String phoneNumber = getStringValueFromCell(row.getCell(0)).trim(); phoneList.add(phoneNumber); &#125; System.out.println(num); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 读取Excel里面Cell内容 * * @param cell * @return */ private static String getStringValueFromCell(XSSFCell cell) &#123; // 单元格内的时间格式 SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd"); // 单元格内的数字类型 DecimalFormat decimalFormat = new DecimalFormat("#.#####"); // 单元格默认为空 String cellValue = ""; if (cell == null) &#123; return cellValue; &#125; // 按类型读取 if (cell.getCellType() == XSSFCell.CELL_TYPE_STRING) &#123; cellValue = cell.getStringCellValue(); &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_NUMERIC) &#123; // 日期转为时间形式 if (DateUtil.isCellDateFormatted(cell)) &#123; double d = cell.getNumericCellValue(); Date date = DateUtil.getJavaDate(d); cellValue = dateFormat.format(date); &#125; else &#123; // 其他转为数字 cellValue = decimalFormat.format((cell.getNumericCellValue())); &#125; &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_BLANK) &#123; cellValue = ""; &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_BOOLEAN) &#123; cellValue = String.valueOf(cell.getBooleanCellValue()); &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_ERROR) &#123; cellValue = ""; &#125; else if (cell.getCellType() == XSSFCell.CELL_TYPE_FORMULA) &#123; cellValue = cell.getCellFormula().toString(); &#125; return cellValue; &#125; /** * 外部接口耗时长，通过多线程增强 * * @param userPhone */ public void sendMsgToPhone(String userPhone) &#123; try &#123; Thread.sleep(SEND_COST_TIME); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("send message to : " + userPhone); &#125; /** * 单线程发送 * * @param phoneList * @return */ private long singleThread(List&lt;String&gt; phoneList) &#123; long start = System.currentTimeMillis(); /*// 直接主线程执行 for (String phoneNumber : phoneList) &#123; threadOperation.sendMsgToPhone(phoneNumber); &#125;*/ SendMsgExtendThread smet = threadOperation.new SendMsgExtendThread(phoneList); smet.start(); long totalTime = System.currentTimeMillis() - start; System.out.println("单线程发送总时间：" + totalTime); return totalTime; &#125; /** * 另外一种分配方式 * * @param phoneList */ private void otherThread(List&lt;String&gt; phoneList) &#123; for (int threadNo = 0; threadNo &lt; 10; threadNo++) &#123; int numbersPerThread = 10; List&lt;String&gt; list = phoneList.subList(threadNo * numbersPerThread, (threadNo * numbersPerThread) + 10); SendMsgExtendThread smet = threadOperation.new SendMsgExtendThread(list); smet.start(); if (list.size() &lt; numbersPerThread) &#123; break; &#125; &#125; &#125; /** * 两个线程发送 * * @param phoneList * @return */ private long twoThreads(List&lt;String&gt; phoneList) &#123; long start = System.currentTimeMillis(); List&lt;String&gt; list1 = phoneList.subList(0, phoneList.size() / 2); List&lt;String&gt; list2 = phoneList.subList(phoneList.size() / 2, phoneList.size()); SendMsgExtendThread smet = threadOperation.new SendMsgExtendThread(list1); smet.start(); SendMsgExtendThread smet1 = threadOperation.new SendMsgExtendThread(list2); smet1.start(); return 0; &#125; /** * 线程池发送 * * @param phoneList * @return */ private void threadPool(List&lt;String&gt; phoneList) &#123; for (int threadNo = 0; threadNo &lt; THREAD_POOL_SIZE; threadNo++) &#123; int numbersPerThread = 10; List&lt;String&gt; list = phoneList.subList(threadNo * numbersPerThread, (threadNo * numbersPerThread) + 10); threadOperation.executorService.execute(threadOperation.new SendMsgExtendThread(list)); &#125; threadOperation.executorService.shutdown(); &#125; /** * 多线程发送 * * @param phoneList * @return */ private void multiThreadSend(List&lt;String&gt; phoneList) &#123; List&lt;Future&lt;Long&gt;&gt; futures = new ArrayList&lt;&gt;(); for (int threadNo = 0; threadNo &lt; THREAD_POOL_SIZE; threadNo++) &#123; int numbersPerThread = 100; List&lt;String&gt; list = phoneList.subList(threadNo * numbersPerThread, (threadNo * numbersPerThread) + 100); Future&lt;Long&gt; future = threadOperation.executorService.submit(threadOperation.new SendMsgImplCallable(list, String.valueOf(threadNo))); futures.add(future); &#125; for (Future&lt;Long&gt; future : futures) &#123; try &#123; System.out.println(future.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; threadOperation.executorService.shutdown(); &#125; public class SendMsgExtendThread extends Thread &#123; private List&lt;String&gt; numberListByThread; public SendMsgExtendThread(List&lt;String&gt; numberList) &#123; numberListByThread = numberList; &#125; @Override public void run() &#123; long startTime = System.currentTimeMillis(); for (int i = 0; i &lt; numberListByThread.size(); i++) &#123; System.out.print("no." + (i + 1)); sendMsgToPhone(numberListByThread.get(i)); &#125; System.out.println("== single thread send " + numberListByThread.size() + "execute time:" + (System.currentTimeMillis() - startTime) + " ms"); &#125; &#125; public class SendMsgImplCallable implements Callable&lt;Long&gt; &#123; private List&lt;String&gt; numberListByThread; private String threadName; public SendMsgImplCallable(List&lt;String&gt; numberList, String threadName) &#123; numberListByThread = numberList; this.threadName = threadName; &#125; @Override public Long call() throws Exception &#123; Long startMills = System.currentTimeMillis(); for (String number : numberListByThread) &#123; sendMsgToPhone(number); &#125; Long endMills = System.currentTimeMillis(); return endMills - startMills; &#125; &#125;&#125; hahaha]]></content>
  </entry>
  <entry>
    <title><![CDATA[Trie 树]]></title>
    <url>%2F2019%2F01%2F24%2Fyuque%2FTrie%20%E6%A0%91%2F</url>
    <content type="text"><![CDATA[2013年09月23日 02:14:27 arhaiyun 阅读数：9467 *Trie 树， *又称字典树，单词查找树。它来源于retrieval(检索)中取中间四个字符构成(读音同try)。用于存储大量的字符串以便支持快速模式匹配。主要应用在信息检索领域。 Trie 有三种结构： 标准trie (standard trie)、压缩trie、后缀trie(suffix trie) 。 最后一种将在《字符串处理4：后缀树》中详细讲，这里只将前两种。 1. 标准Trie (standard trie) *标准 Trie树的结构 *： 所有含有公共前缀的字符串将挂在树中同一个结点下。实际上trie简明的存储了存在于串集合中的所有公共前缀。 假如有这样一个字符串集合X{bear,bell,bid,bull,buy,sell,stock,stop}。它的标准Trie树如下图： 上图（蓝色圆形结点为内部结点，红色方形结点为外部结点），我们可以很清楚的看到字符串集合X构造的Trie树结构。其中从根结点到红色方框叶子节点所经历的所有字符组成的串就是字符串集合X中的一个串。 注意这里有一个问题： 如果X集合中有一个串是另一个串的前缀呢？ 比如，X集合中加入串bi。那么上图的Trie树在绿色箭头所指的内部结点i 就应该也标记成红色方形结点。这样话，一棵树的枝干上将出现两个连续的叶子结点(这是不合常理的)。 也就是说字符串集合X中不存在一个串是另外一个串的前缀 。如何满足这个要求呢？我们可以在X中的每个串后面加入一个特殊字符$(这个字符将不会出现在字母表中)。这样，集合X{bear$、bell$、…. bi$、bid$}一定会满足这个要求。 总结：一个存储长度为n，来自大小为d的字母表中s个串的集合X的标准trie具有性质如下： (1) 树中每个内部结点至多有d个子结点。 (2) 树有s个外部结点。 (3) 树的高度等于X中最长串的长度。 (4) 树中的结点数为O(n)。 标准 Trie树的查找 对于英文单词的查找，我们完全可以在内部结点中建立26个元素组成的指针数组。如果要查找a，只需要在内部节点的指针数组中找第0个指针即可(b=第1个指针，随机定位)。时间复杂度为O(1)。 查找过程：假如我们要在上面那棵Trie中查找字符串bull (b-u-l-l)。 (1) 在root结点中查找第(‘b’-‘a’=1)号孩子指针，发现该指针不为空，则定位到第1号孩子结点处——b结点。 (2) 在b结点中查找第(‘u’-‘a’=20)号孩子指针，发现该指针不为空，则定位到第20号孩子结点处——u结点。 (3) … 一直查找到叶子结点出现特殊字符’$’位置，表示找到了bull字符串 如果在查找过程中终止于内部结点，则表示没有找到待查找字符串。 效率：对于有n个英文字母的串来说，在内部结点中定位指针所需要花费O(d)时间，d为字母表的大小，英文为26。由于在上面的算法中内部结点指针定位使用了数组随机存储方式，因此时间复杂度降为了O(1)。但是如果是中文字，下面在实际应用中会提到。因此我们在这里还是用O(d)。 查找成功的时候恰好走了一条从根结点到叶子结点的路径。因此时间复杂度为O(d*n)。 但是，当查找集合X中所有字符串两两都不共享前缀时，trie中出现最坏情况。除根之外，所有内部结点都自由一个子结点。此时的查找时间复杂度蜕化为O(d*(n^2)) 标准 Trie树的Java代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150/*StandarTire.javaTrie 树， 又称字典树，单词查找树。它来源于retrieval(检索)中取中间四个字符构成(读音同try)。用于存储大量的字符串以便支持快速模式匹配。主要应用在信息检索领域。@author arhaiyundate:2013/09/23*/ import java.util.*; enum NodeKind&#123;LN, BN&#125;; /***Trie node*/class TrieNode&#123; char key; TrieNode[] points = null; NodeKind kind = null;&#125; /*** Branch node*/class BranchNode extends TrieNode&#123; BranchNode(char k) &#123; super.key = k; super.kind = NodeKind.BN; super.points = new TrieNode[27]; &#125;&#125; /*** Leaf node*/class LeafNode extends TrieNode&#123; LeafNode(char k) &#123; super.key = k; super.kind = NodeKind.LN; &#125;&#125; public class StandardTrie&#123; //Create root node TrieNode root = new BranchNode(' '); //[1].Insert a word into tire tree public void insert(String words) &#123; TrieNode curNode = root; //add '$' as an end symbol words = words + "$"; char[] chars = words.toCharArray(); for(int i = 0; i &lt; chars.length; i++) &#123; if(chars[i] == '$') &#123; curNode.points[26] = new LeafNode('$'); &#125; else &#123; int pSize = chars[i] - 'a'; // If not exists creat a new branch node if(curNode.points[pSize] == null) &#123; curNode.points[pSize] = new BranchNode(chars[i]); &#125; curNode = curNode.points[pSize]; &#125; &#125; &#125; //[2].Check if a word is in tire tree public boolean fullMatch(String words) &#123; TrieNode curNode = root; char[] chars = words.toCharArray(); for(int i = 0; i &lt; chars.length; i++) &#123; int pSize = chars[i] - 'a'; System.out.print(chars[i]+"-&gt;"); if(curNode.points[pSize] == null) return false; curNode = curNode.points[pSize]; &#125; if(curNode.points[26] != null &amp;&amp; curNode.points[26].key == '$') return true; return false; &#125; //[3].preorder root traverse private void preorderTraverse(TrieNode curNode) &#123; if(curNode != null) &#123; System.out.print(curNode.key); if(curNode.kind == NodeKind.BN) &#123; for(TrieNode node : curNode.points) &#123; preorderTraverse(node); &#125; &#125; else System.out.println(); &#125; &#125; //[4].Get root node public TrieNode getRoot() &#123; return this.root; &#125; public static void main(String[] args) &#123; StandardTrie trie = new StandardTrie(); trie.insert("amazon"); trie.insert("yahoo"); trie.insert("haiyun"); trie.insert("baidu"); trie.insert("alibaba"); trie.insert("offer"); trie.insert("stock"); trie.insert("stop"); trie.preorderTraverse(trie.getRoot()); System.out.println(trie.fullMatch("yahoo")); System.out.println(trie.fullMatch("yaho")); System.out.println(trie.fullMatch("baidu")); System.out.println(trie.fullMatch("alibaba")); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java 泛型总结（三）：通配符的使用 - Coding - SegmentFault 思否]]></title>
    <url>%2F2019%2F01%2F22%2Fyuque%2FJava%20%E6%B3%9B%E5%9E%8B%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E9%80%9A%E9%85%8D%E7%AC%A6%E7%9A%84%E4%BD%BF%E7%94%A8%20-%20Coding%20-%20SegmentFault%20%E6%80%9D%E5%90%A6%2F</url>
    <content type="text"><![CDATA[简介前两篇文章介绍了泛型的基本用法、类型擦除以及泛型数组。在泛型的使用中，还有个重要的东西叫通配符，本文介绍通配符的使用。 这个系列的另外两篇文章： Java 泛型总结（一）：基本用法与类型擦除 Java 泛型总结（二）：泛型与数组 数组的协变在了解通配符之前，先来了解一下数组。Java 中的数组是协变的，什么意思？看下面的例子： class Fruit {} class Apple extends Fruit {} class Jonathan extends Apple {} class Orange extends Fruit {} public class CovariantArrays { public static void main(String[] args) { Fruit[] fruit = new Apple[10]; fruit[0] = new Apple(); fruit[1] = new Jonathan(); try { fruit[0] = new Fruit(); } catch(Exception e) { System.out.println(e); } try { fruit[0] = new Orange(); } catch(Exception e) { System.out.println(e); } } } main 方法中的第一行，创建了一个 Apple 数组并把它赋给 Fruit 数组的引用。这是有意义的，Apple 是 Fruit 的子类，一个 Apple 对象也是一种 Fruit 对象，所以一个 Apple 数组也是一种 Fruit 的数组。这称作数组的协变，Java 把数组设计为协变的，对此是有争议的，有人认为这是一种缺陷。 尽管 Apple[] 可以 “向上转型” 为 Fruit[]，但数组元素的实际类型还是 Apple，我们只能向数组中放入 Apple或者 Apple 的子类。在上面的代码中，向数组中放入了 Fruit 对象和 Orange 对象。对于编译器来说，这是可以通过编译的，但是在运行时期，JVM 能够知道数组的实际类型是 Apple[]，所以当其它对象加入数组的时候就会抛出异常。 泛型设计的目的之一是要使这种运行时期的错误在编译期就能发现，看看用泛型容器类来代替数组会发生什么： ArrayList&lt;Fruit&gt; flist = new ArrayList&lt;Apple&gt;();上面的代码根本就无法编译。当涉及到泛型时， 尽管 Apple 是 Fruit 的子类型，但是 ArrayList&lt;Apple&gt; 不是 ArrayList&lt;Fruit&gt; 的子类型，泛型不支持协变。 使用通配符从上面我们知道，List&lt;Number&gt; list = ArrayList&lt;Integer&gt; 这样的语句是无法通过编译的，尽管 Integer 是 Number 的子类型。那么如果我们确实需要建立这种 “向上转型” 的关系怎么办呢？这就需要通配符来发挥作用了。 上边界限定通配符利用 &lt;? extends Fruit&gt; 形式的通配符，可以实现泛型的向上转型： public class GenericsAndCovariance { public static void main(String[] args) { List&lt;? extends Fruit&gt; flist = new ArrayList&lt;Apple&gt;(); flist.add(null); Fruit f = flist.get(0); } }上面的例子中， flist 的类型是 List&lt;? extends Fruit&gt;，我们可以把它读作：一个类型的 List， 这个类型可以是继承了 Fruit 的某种类型。注意，这并不是说这个 List 可以持有 Fruit 的任意类型。通配符代表了一种特定的类型，它表示 “某种特定的类型，但是 flist 没有指定”。这样不太好理解，具体针对这个例子解释就是，flist 引用可以指向某个类型的 List，只要这个类型继承自 Fruit，可以是 Fruit 或者 Apple，比如例子中的 new ArrayList&lt;Apple&gt;，但是为了向上转型给 flist，flist 并不关心这个具体类型是什么。 如上所述，通配符 List&lt;? extends Fruit&gt; 表示某种特定类型 ( Fruit 或者其子类 ) 的 List，但是并不关心这个实际的类型到底是什么，反正是 Fruit 的子类型，Fruit 是它的上边界。那么对这样的一个 List 我们能做什么呢？其实如果我们不知道这个 List 到底持有什么类型，怎么可能安全的添加一个对象呢？在上面的代码中，向 flist 中添加任何对象，无论是 Apple 还是 Orange 甚至是 Fruit 对象，编译器都不允许，唯一可以添加的是 null。所以如果做了泛型的向上转型 (List&lt;? extends Fruit&gt; flist = new ArrayList&lt;Apple&gt;())，那么我们也就失去了向这个 List 添加任何对象的能力，即使是 Object 也不行。 另一方面，如果调用某个返回 Fruit 的方法，这是安全的。因为我们知道，在这个 List 中，不管它实际的类型到底是什么，但肯定能转型为 Fruit，所以编译器允许返回 Fruit。 了解了通配符的作用和限制后，好像任何接受参数的方法我们都不能调用了。其实倒也不是，看下面的例子： public class CompilerIntelligence { public static void main(String[] args) { List&lt;? extends Fruit&gt; flist = Arrays.asList(new Apple()); Apple a = (Apple)flist.get(0); flist.contains(new Apple()); flist.indexOf(new Apple()); } }在上面的例子中，flist 的类型是 List&lt;? extends Fruit&gt;，泛型参数使用了受限制的通配符，所以我们失去了向其中加入任何类型对象的例子，最后一行代码无法编译。 但是 flist 却可以调用 contains 和 indexOf 方法，它们都接受了一个 Apple 对象做参数。如果查看 ArrayList 的源代码，可以发现 add() 接受一个泛型类型作为参数，但是 contains 和 indexOf 接受一个 Object 类型的参数，下面是它们的方法签名： public boolean add(E e) public boolean contains(Object o) public int indexOf(Object o)所以如果我们指定泛型参数为 &lt;? extends Fruit&gt; 时，add() 方法的参数变为 ? extends Fruit，编译器无法判断这个参数接受的到底是 Fruit 的哪种类型，所以它不会接受任何类型。 然而，contains 和 indexOf 的类型是 Object，并没有涉及到通配符，所以编译器允许调用这两个方法。这意味着一切取决于泛型类的编写者来决定那些调用是 “安全” 的，并且用 Object 作为这些安全方法的参数。如果某些方法不允许类型参数是通配符时的调用，这些方法的参数应该用类型参数，比如 add(E e)。 当我们自己编写泛型类时，上面介绍的就有用了。下面编写一个 Holder 类： public class Holder&lt;T&gt; { private T value; public Holder() {} public Holder(T val) { value = val; } public void set(T val) { value = val; } public T get() { return value; } public boolean equals(Object obj) { return value.equals(obj); } public static void main(String[] args) { Holder&lt;Apple&gt; Apple = new Holder&lt;Apple&gt;(new Apple()); Apple d = Apple.get(); Apple.set(d); Holder&lt;? extends Fruit&gt; fruit = Apple; Fruit p = fruit.get(); d = (Apple)fruit.get(); try { Orange c = (Orange)fruit.get(); } catch(Exception e) { System.out.println(e); } System.out.println(fruit.equals(d)); } } 在 Holer 类中，set() 方法接受类型参数 T 的对象作为参数，get() 返回一个 T 类型，而 equals() 接受一个 Object 作为参数。fruit 的类型是 Holder&lt;? extends Fruit&gt;，所以set()方法不会接受任何对象的添加，但是 equals() 可以正常工作。 下边界限定通配符通配符的另一个方向是 “超类型的通配符“: ? super T，T 是类型参数的下界。使用这种形式的通配符，我们就可以 ”传递对象” 了。还是用例子解释： public class SuperTypeWildcards { static void writeTo(List&lt;? super Apple&gt; apples) { apples.add(new Apple()); apples.add(new Jonathan()); } }writeTo 方法的参数 apples 的类型是 List&lt;? super Apple&gt;，它表示某种类型的 List，这个类型是 Apple 的基类型。也就是说，我们不知道实际类型是什么，但是这个类型肯定是 Apple 的父类型。因此，我们可以知道向这个 List 添加一个 Apple 或者其子类型的对象是安全的，这些对象都可以向上转型为 Apple。但是我们不知道加入 Fruit 对象是否安全，因为那样会使得这个 List 添加跟 Apple 无关的类型。 在了解了子类型边界和超类型边界之后，我们就可以知道如何向泛型类型中 “写入” ( 传递对象给方法参数) 以及如何从泛型类型中 “读取” ( 从方法中返回对象 )。下面是一个例子： public class Collections { public static &lt;T&gt; void copy(List&lt;? super T&gt; dest, List&lt;? extends T&gt; src) { for (int i=0; i&lt;src.size(); i++) dest.set(i,src.get(i)); } }src 是原始数据的 List，因为要从这里面读取数据，所以用了上边界限定通配符：&lt;? extends T&gt;，取出的元素转型为 T。dest 是要写入的目标 List，所以用了下边界限定通配符：&lt;? super T&gt;，可以写入的元素类型是 T 及其子类型。 无边界通配符还有一种通配符是无边界通配符，它的使用形式是一个单独的问号：List&lt;?&gt;，也就是没有任何限定。不做任何限制，跟不用类型参数的 List 有什么区别呢？ List&lt;?&gt; list 表示 list 是持有某种特定类型的 List，但是不知道具体是哪种类型。那么我们可以向其中添加对象吗？当然不可以，因为并不知道实际是哪种类型，所以不能添加任何类型，这是不安全的。而单独的 List list ，也就是没有传入泛型参数，表示这个 list 持有的元素的类型是 Object，因此可以添加任何类型的对象，只不过编译器会有警告信息。 总结通配符的使用可以对泛型参数做出某些限制，使代码更安全，对于上边界和下边界限定的通配符总结如下： 使用 List&lt;? extends C&gt; list 这种形式，表示 list 可以引用一个 ArrayList ( 或者其它 List 的 子类 ) 的对象，这个对象包含的元素类型是 C 的子类型 ( 包含 C 本身）的一种。 使用 List&lt;? super C&gt; list 这种形式，表示 list 可以引用一个 ArrayList ( 或者其它 List 的 子类 ) 的对象，这个对象包含的元素就类型是 C 的超类型 ( 包含 C 本身 ) 的一种。 大多数情况下泛型的使用比较简单，但是如果自己编写支持泛型的代码需要对泛型有深入的了解。这几篇文章介绍了泛型的基本用法、类型擦除、泛型数组以及通配符的使用，涵盖了最常用的要点，泛型的总结就写到这里。 参考 Java 编程思想 如果我的文章对您有帮助，不妨点个赞支持一下(^\^)_]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java 泛型总结（一）：基本用法与类型擦除 - Coding - SegmentFault 思否]]></title>
    <url>%2F2019%2F01%2F22%2Fyuque%2FJava%20%E6%B3%9B%E5%9E%8B%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%E4%B8%8E%E7%B1%BB%E5%9E%8B%E6%93%A6%E9%99%A4%20-%20Coding%20-%20SegmentFault%20%E6%80%9D%E5%90%A6%2F</url>
    <content type="text"><![CDATA[简介Java 在 1.5 引入了泛型机制，泛型本质是参数化类型，也就是说变量的类型是一个参数，在使用时再指定为具体类型。泛型可以用于类、接口、方法，通过使用泛型可以使代码更简单、安全。然而 Java 中的泛型使用了类型擦除，所以只是伪泛型。这篇文章对泛型的使用以及存在的问题做个总结，主要参考自 《Java 编程思想》。 这个系列的另外两篇文章： Java 泛型总结（二）：泛型与数组 Java 泛型总结（三）：通配符的使用 基本用法 泛型类如果有一个类 Holder 用于包装一个变量，这个变量的类型可能是任意的，怎么编写 Holder 呢？在没有泛型之前可以这样： 123456789101112131415161718192021public class Holder1 &#123; private Object a; public Holder1(Object a) &#123; this.a = a; &#125; public void set(Object a) &#123; this.a = a; &#125; public Object get()&#123; return a; &#125; public static void main(String[] args) &#123; Holder1 holder1 = new Holder1("not Generic"); String s = (String) holder1.get(); holder1.set(1); Integer x = (Integer) holder1.get(); &#125;&#125;j 在 Holder1 中，有一个用 Object 引用的变量。因为任何类型都可以向上转型为 Object，所以这个 Holder 可以接受任何类型。在取出的时候 Holder 只知道它保存的是一个 Object 对象，所以要强制转换为对应的类型。在 main 方法中， holder1 先是保存了一个字符串，也就是 String 对象，接着又变为保存一个 Integer 对象(参数 1 会自动装箱)。从 Holder 中取出变量时强制转换已经比较麻烦，这里还要记住不同的类型，要是转错了就会出现运行时异常。 下面看看 Holder 的泛型版本： 123456789101112131415161718192021222324public class Holder2&lt;T&gt; &#123; private T a; public Holder2(T a) &#123; this.a = a; &#125; public T get() &#123; return a; &#125; public void set(T a) &#123; this.a = a; &#125; public static void main(String[] args) &#123; Holder2&lt;String&gt; holder2 = new Holder2&lt;&gt;("Generic"); String s = holder2.get(); holder2.set("test"); holder2.set(1); // 编译不通过 &#125;&#125; 在 Holder2 中， 变量 a 是一个参数化类型 T，T`` 只是一个标识，用其它字母也是可以的。创建 Holder2 对象的时候，在尖括号中传入了参数 T 的类型，那么在这个对象中，所有出现 T 的地方相当于都用 String 替换了。现在的 get 的取出来的不是 Object ，而是 String 对象，因此不需要类型转换。另外，当调用 set 时，只能传入 String 类型，否则编译无法通过。这就保证了 holder2 中的类型安全，避免由于不小心传入错误的类型。 通过上面的例子可以看出泛使得代码更简便、安全。引入泛型之后，Java 库的一些类，比如常用的容器类也被改写为支持泛型，我们使用的时候都会传入参数类型，如：ArrayList&lt;Integer&gt; list = ArrayList&lt;&gt;();。 泛型方法泛型不仅可以针对类，还可以单独使某个方法是泛型的，举个例子： 12345678910111213141516public class GenericMethod &#123; // 如果在参数中使用了泛型，那么在函数的签名上要用尖括号表现出泛型 public &lt;K,V&gt; void f(K k,V v) &#123; System.out.println(k.getClass().getSimpleName()); System.out.println(v.getClass().getSimpleName()); &#125; public static void main(String[] args) &#123; GenericMethod gm = new GenericMethod(); gm.f(new Integer(0),new String("generic")); &#125;&#125;代码输出： Integer String GenericMethod 类本身不是泛型的，创建它的对象的时候不需要传入泛型参数，但是它的方法 f 是泛型方法。在返回类型之前是它的参数标识 &lt;K,V&gt;，注意这里有两个泛型参数，所以泛型参数可以有多个。 调用泛型方法时可以不显式传入泛型参数，上面的调用就没有。这是因为编译器会使用参数类型推断，根据传入的实参的类型 (这里是 integer 和 String) 推断出 K 和 V 的类型。 类型擦除 什么是类型擦除Java 的泛型使用了类型擦除机制，这个引来了很大的争议，以至于 Java 的泛型功能受到限制，只能说是”伪泛型“。什么叫类型擦除呢？简单的说就是，类型参数只存在于编译期，在运行时，Java 的虚拟机 ( JVM ) 并不知道泛型的存在。先看个例子： 1234567public class ErasedTypeEquivalence &#123; public static void main(String[] args) &#123; Class c1 = new ArrayList&lt;String&gt;().getClass(); Class c2 = new ArrayList&lt;Integer&gt;().getClass(); System.out.println(c1 == c2); &#125;&#125; 上面的代码有两个不同的 ArrayList：ArrayList&lt;Integer&gt; 和 ArrayList&lt;String&gt;。在我们看来它们的参数化类型不同，一个保存整性，一个保存字符串。但是通过比较它们的 Class 对象，上面的代码输出是 true。这说明在 JVM 看来它们是同一个类。而在 C++、C# 这些支持真泛型的语言中，它们就是不同的类。 泛型参数会擦除到它的第一个边界，比如说上面的 Holder2 类，参数类型是一个单独的 T，那么就擦除到 Object,相当于所有出现 T 的地方都用 Object 替换。所以在 JVM 看来，保存的变量 a 还是 Object 类型。之所以取出来自动就是我们传入的参数类型，这是因为编译器在编译生成的字节码文件中插入了类型转换的代码，不需要我们手动转型了。如果参数类型有边界那么就擦除到它的第一个边界，这个下一节再说。简而言之：在代码层面，虽有泛型的存在，但是在运行时候的jvm层面却感受不到泛型的存在 擦除带来的问题擦除会出现一些问题，下面是一个例子： 12345678910111213141516171819202122class HasF &#123; public void f() &#123; System.out.println("HasF.f()"); &#125;&#125;public class Manipulator&lt;T&gt; &#123; private T obj; public Manipulator(T obj) &#123; this.obj = obj; &#125; public void manipulate() &#123; obj.f(); &#125; public static void main(String[] args) &#123; HasF hasF = new HasF(); Manipulator&lt;HasF&gt; manipulator = new Manipulator&lt;&gt;(hasF); manipulator.manipulate(); &#125;&#125; 上面的 Manipulator 是一个泛型类，内部用一个泛型化的变量 obj，在 manipulate 方法中，调用了 obj 的方法 f()，但是这行代码无法编译。因为类型擦除，编译器不确定 obj 是否有 f() 方法。解决这个问题的方法是给 T 一个边界: 12345class Manipulator2&lt;T extends HasF&gt; &#123; private T obj; public Manipulator2(T x) &#123; obj = x; &#125; public void manipulate() &#123; obj.f(); &#125;&#125; 现在 T 的类型是 &lt;T extends HasF&gt;，这表示 T必须是 HasF 或者 HasF`` 的导出类型。这样，调用 f() 方法才安全。HasF 就是 T 的边界，因此通过类型擦除后，所有出现 T 的地方都用 HasF 替换。这样编译器就知道 obj 是有方法 f() 的。 但是这样就抵消了泛型带来的好处，上面的类完全可以改成这样： 12345class Manipulator3 &#123; private HasF obj; public Manipulator3(HasF x) &#123; obj = x; &#125; public void manipulate() &#123; obj.f(); &#125;&#125; 所以泛型只有在比较复杂的类中才体现出作用。但是像 &lt;T extends HasF&gt; 这种形式的东西不是完全没有意义的。如果类中有一个返回 T 类型的方法，泛型就有用了，因为这样会返回准确类型。比如下面的例子： 12345class ReturnGenericType&lt;T extends HasF&gt; &#123; private T obj; public ReturnGenericType(T x) &#123; obj = x; &#125; public T get() &#123; return obj; &#125;&#125; 这里的 get() 方法返回的是泛型参数的准确类型，而不是 HasF。 类型擦除的补偿类型擦除导致泛型丧失了一些功能，任何在运行期需要知道确切类型的代码都无法工作。比如下面的例子： 123456789public class Erased&lt;T&gt; &#123; private final int SIZE = 100; public static void f(Object arg) &#123; if(arg instanceof T) &#123;&#125; T var = new T(); T[] array = new T[SIZE]; T[] array = (T)new Object[SIZE]; &#125;&#125; 通过 new T() 创建对象是不行的，一是由于类型擦除，二是由于编译器不知道 T 是否有默认的构造器。一种解决的办法是传递一个工厂对象并且通过它创建新的实例。 123456789101112131415161718192021222324252627interface FactoryI&lt;T&gt; &#123; T create();&#125;class Foo2&lt;T&gt; &#123; private T x; public &lt;F extends FactoryI&lt;T&gt;&gt; Foo2(F factory) &#123; x = factory.create(); &#125;&#125;class IntegerFactory implements FactoryI&lt;Integer&gt; &#123; public Integer create() &#123; return new Integer(0); &#125;&#125;class Widget &#123; public static class Factory implements FactoryI&lt;Widget&gt; &#123; public Widget create() &#123; return new Widget(); &#125; &#125;&#125;public class FactoryConstraint &#123; public static void main(String[] args) &#123; new Foo2&lt;Integer&gt;(new IntegerFactory()); new Foo2&lt;Widget&gt;(new Widget.Factory()); &#125;&#125; 另一种解决的方法是利用模板设计模式： 123456789101112131415161718abstract class GenericWithCreate&lt;T&gt; &#123; final T element; GenericWithCreate() &#123; element = create(); &#125; abstract T create();&#125;class X &#123;&#125;class Creator extends GenericWithCreate&lt;X&gt; &#123; X create() &#123; return new X(); &#125; void f() &#123; System.out.println(element.getClass().getSimpleName()); &#125;&#125;public class CreatorGeneric &#123; public static void main(String[] args) &#123; Creator c = new Creator(); c.f(); &#125;&#125; 具体类型的创建放到了子类继承父类时，在 create 方法中创建实际的类型并返回。 总结本文介绍了 Java 泛型的使用，以及类型擦除相关的问题。一般情况下泛型的使用比较简单，但是某些情况下，尤其是自己编写使用泛型的类或者方法时要注意类型擦除的问题。接下来会介绍数组与泛型的关系以及通配符的使用，有兴趣的读者可进入下一篇：Java 泛型总结（二）：泛型与数组。 参考 Java 编程思想 如果我的文章对您有帮助，不妨点个赞支持一下(____)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux下五种IO模型]]></title>
    <url>%2F2019%2F01%2F22%2Fyuque%2FLinux%E4%B8%8B%E4%BA%94%E7%A7%8DIO%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在Linux(UNIX)操作系统中，共有五种IO模型，分别是：阻塞IO模型非阻塞IO模型IO复用模型信号驱动IO模型异步IO模型。 到底什么是IO我们常说的IO，指的是文件的输入和输出，但是在操作系统层面是如何定义IO的呢？到底什么样的过程可以叫做是一次IO呢？拿一次磁盘文件读取为例，我们要读取的文件是存储在磁盘上的，我们的目的是把它读取到内存中。可以把这个步骤简化成把数据从硬件（硬盘）中读取到用户空间中。其实真正的文件读取还涉及到缓存等细节，这里就不展开讲述了。关于用户空间、内核空间以及硬件等的关系如果读者不理解的话，可以通过钓鱼的例子理解。钓鱼的时候，刚开始鱼是在鱼塘里面的，我们的钓鱼动作的最终结束标志是鱼从鱼塘中被我们钓上来，放入鱼篓中。这里面的鱼塘就可以映射成磁盘，中间过渡的鱼钩可以映射成内核空间，最终放鱼的鱼篓可以映射成用户空间。一次完整的钓鱼（IO）操作，是鱼（文件）从鱼塘（硬盘）中转移（拷贝）到鱼篓（用户空间）的过程。 阻塞IO模型我们钓鱼的时候，有一种方式比较惬意，比较轻松，那就是我们坐在鱼竿面前，这个过程中我们什么也不做，双手一直把着鱼竿，就静静的等着鱼儿咬钩。一旦手上感受到鱼的力道，就把鱼钓起来放入鱼篓中。然后再钓下一条鱼。映射到Linux操作系统中，这就是一种最简单的IO模型，即阻塞IO。 阻塞 I/O 是最简单的 I/O 模型，一般表现为进程或线程等待某个条件，如果条件不满足，则一直等下去。条件满足，则进行下一步操作。 应用进程通过系统调用 recvfrom 接收数据，但由于内核还未准备好数据报，应用进程就会阻塞住，直到内核准备好数据报，recvfrom 完成数据报复制工作，应用进程才能结束阻塞状态。这种钓鱼方式相对来说比较简单，对于钓鱼的人来说，不需要什么特制的鱼竿，拿一根够长的木棍就可以悠闲的开始钓鱼了（实现简单）。缺点就是比较耗费时间，比较适合那种对鱼的需求量小的情况（并发低，时效性要求低）。 非阻塞IO模型我们钓鱼的时候，在等待鱼儿咬钩的过程中，我们可以做点别的事情，比如玩一把王者荣耀、看一集《延禧攻略》等等。但是，我们要时不时的去看一下鱼竿，一旦发现有鱼儿上钩了，就把鱼钓上来。映射到Linux操作系统中，这就是非阻塞的IO模型。应用进程与内核交互，目的未达到之前，不再一味的等着，而是直接返回。然后通过轮询的方式，不停的去问内核数据准备有没有准备好。如果某一次轮询发现数据已经准备好了，那就把数据拷贝到用户空间中。 应用进程通过 recvfrom 调用不停的去和内核交互，直到内核准备好数据。如果没有准备好，内核会返回error，应用进程在得到error后，过一段时间再发送recvfrom请求。在两次发送请求的时间段，进程可以先做别的事情。这种方式钓鱼，和阻塞IO比，所使用的工具没有什么变化，但是钓鱼的时候可以做些其他事情，增加时间的利用率。 信号驱动IO模型我们钓鱼的时候，为了避免自己一遍一遍的去查看鱼竿，我们可以给鱼竿安装一个报警器。当有鱼儿咬钩的时候立刻报警。然后我们再收到报警后，去把鱼钓起来。映射到Linux操作系统中，这就是信号驱动IO。应用进程在读取文件时通知内核，如果某个 socket 的某个事件发生时，请向我发一个信号。在收到信号后，信号对应的处理函数会进行后续处理。 应用进程预先向内核注册一个信号处理函数，然后用户进程返回，并且不阻塞，当内核数据准备就绪时会发送一个信号给进程，用户进程便在信号处理函数中开始把数据拷贝的用户空间中。这种方式钓鱼，和前几种相比，所使用的工具有了一些变化，需要有一些定制（实现复杂）。但是钓鱼的人就可以在鱼儿咬钩之前彻底做别的事儿去了。等着报警器响就行了。 IO复用模型我们钓鱼的时候，为了保证可以最短的时间钓到最多的鱼，我们同一时间摆放多个鱼竿，同时钓鱼。然后哪个鱼竿有鱼儿咬钩了，我们就把哪个鱼竿上面的鱼钓起来。映射到Linux操作系统中，这就是IO复用模型。多个进程的IO可以注册到同一个管道上，这个管道会统一和内核进行交互。当管道中的某一个请求需要的数据准备好之后，进程再把对应的数据拷贝到用户空间中。 IO多路转接是多了一个select函数，多个进程的IO可以注册到同一个select上，当用户进程调用该select，select会监听所有注册好的IO，如果所有被监听的IO需要的数据都没有准备好时，select调用进程会阻塞。当任意一个IO所需的数据准备好之后，select调用就会返回，然后进程在通过recvfrom来进行数据拷贝。这里的IO复用模型，并没有向内核注册信号处理函数，所以，他并不是非阻塞的。进程在发出select后，要等到select监听的所有IO操作中至少有一个需要的数据准备好，才会有返回，并且也需要再次发送请求去进行文件的拷贝。这种方式的钓鱼，通过增加鱼竿的方式，可以有效的提升效率。 为什么以上四种都是同步的我们说阻塞IO模型、非阻塞IO模型、IO复用模型和信号驱动IO模型都是同步的IO模型。原因是因为，无论以上那种模型，真正的数据拷贝过程，都是同步进行的。信号驱动难道不是异步的么？ 信号驱动，内核是在数据准备好之后通知进程，然后进程再通过recvfrom操作进行数据拷贝。我们可以认为数据准备阶段是异步的，但是，数据拷贝操作是同步的。所以，整个IO过程也不能认为是异步的。 我们把钓鱼过程，可以拆分为两个步骤：1、鱼咬钩（数据准备）。2、把鱼钓起来放进鱼篓里（数据拷贝）。无论以上提到的哪种钓鱼方式，在第二步，都是需要人主动去做的，并不是鱼竿自己完成的。所以，这个钓鱼过程其实还是同步进行的。 烧水的报警器一响，整个烧水过程就完成了。水已经是开水了。钓鱼的报警器一响，只能说明鱼儿已经咬钩了，但是还没有真正的钓上来。 所以 ，使用带有报警器的水壶烧水，烧水过程是异步的。而使用带有报警器的鱼竿钓鱼，钓鱼的过程还是同步的。 异步IO模型我们钓鱼的时候，采用一种高科技钓鱼竿，即全自动钓鱼竿。可以自动感应鱼上钩，自动收竿，更厉害的可以自动把鱼放进鱼篓里。然后，通知我们鱼已经钓到了，他就继续去钓下一条鱼去了。映射到Linux操作系统中，这就是异步IO模型。应用进程把IO请求传给内核后，完全由内核去操作文件拷贝。内核完成相关操作后，会发信号告诉应用进程本次IO已经完成。 用户进程发起aio_read操作之后，给内核传递描述符、缓冲区指针、缓冲区大小等，告诉内核当整个操作完成时，如何通知进程，然后就立刻去做其他事情了。当内核收到aio_read后，会立刻返回，然后内核开始等待数据准备，数据准备好以后，直接把数据拷贝到用户控件，然后再通知进程本次IO已经完成。这种方式的钓鱼，无疑是最省事儿的。啥都不需要管，只需要交给鱼竿就可以了。 5种IO模型对比 作者：漫话编程_公众号mhcoding链接：https://juejin.im/post/5b94e93b5188255c672e901e来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
  </entry>
  <entry>
    <title><![CDATA[同步、异步、阻塞、非阻塞]]></title>
    <url>%2F2019%2F01%2F22%2Fyuque%2F%E5%90%8C%E6%AD%A5%E3%80%81%E5%BC%82%E6%AD%A5%E3%80%81%E9%98%BB%E5%A1%9E%E3%80%81%E9%9D%9E%E9%98%BB%E5%A1%9E%2F</url>
    <content type="text"><![CDATA[什么是同步和异步说到烧水，我们都是通过热水壶来烧水的。在很久之前，科技还没有这么发达的时候，如果我们要烧水，需要把水壶放到火炉上，我们通过观察水壶内的水的沸腾程度来判断水有没有烧开。随着科技的发展，现在市面上的水壶都有了提醒功能，当我们把水壶插电之后，水壶水烧开之后会通过声音提醒我们水开了。对于烧水这件事儿来说，传统水壶的烧水就是同步的，高科技水壶的烧水就是异步的。同步请求，A调用B，B的处理是同步的，在处理完之前他不会通知A，只有处理完之后才会明确的通知A。异步请求，A调用B，B的处理是异步的，B在接到请求后先告诉A我已经接到请求了，然后异步去处理，处理完之后通过回调等方式再通知A。所以说，同步和异步最大的区别就是被调用方的执行方式和返回时机。同步指的是被调用方做完事情之后再返回，异步指的是被调用方先返回，然后再做事情，做完之后再想办法通知调用方。 什么是阻塞和非阻塞还是那个烧水的例子，当你把水放到水壶里面，按下开关后，你可以坐在水壶前面，别的事情什么都不做，一直等着水烧好。你还可以先去客厅看电视，等着水开就好了。对于你来说，坐在水壶前面等就是阻塞的，去客厅看电视等着水开就是非阻塞的。阻塞请求，A调用B，A一直等着B的返回，别的事情什么也不干。非阻塞请求，A调用B，A不用一直等着B的返回，先去忙别的事情了。所以说，阻塞和非阻塞最大的区别就是在被调用方返回结果之前的这段时间内，调用方是否一直等待。阻塞指的是调用方一直等待别的事情什么都不做。非阻塞指的是调用方先去忙别的事情。 阻塞、非阻塞和同步、异步的区别首先，前面已经提到过，阻塞、非阻塞和同步、异步其实针对的对象是不一样的。阻塞、非阻塞说的是调用者，同步、异步说的是被调用者。有人认为阻塞和同步是一回事儿，非阻塞和异步是一回事。但是这是不对的。先来看同步场景中是如何包含阻塞和非阻塞情况的。我们是用传统的水壶烧水。在水烧开之前我们一直做在水壶前面，等着水开。这就是阻塞的。我们是用传统的水壶烧水。在水烧开之前我们先去客厅看电视了，但是水壶不会主动通知我们，需要我们时不时的去厨房看一下水有没有烧开。这就是非阻塞的。再来看异步场景中是如何包含阻塞和非阻塞情况的。我们是用带有提醒功能的水壶烧水。在水烧发出提醒之前我们一直做在水壶前面，等着水开。这就是阻塞的。我们是用带有提醒功能的水壶烧水。在水烧发出提醒之前我们先去客厅看电视了，等水壶发出声音提醒我们。这就是非阻塞的。 Java中的三种IO模型在Java语言中，一共提供了三种IO模型，分别是阻塞IO（BIO）、非阻塞IO（NIO）、异步IO（AIO）。这里面的BIO和NIO都是同步的IO模型，即同步阻塞IO和同步非阻塞IO，异步IO指的是异步非阻塞IO。BIO （Blocking I/O）：同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。NIO （New I/O）：同时支持阻塞与非阻塞模式，但主要是使用同步非阻塞IO。AIO （Asynchronous I/O）：异步非阻塞I/O模型。 作者：漫话编程_公众号mhcoding链接：https://juejin.im/post/5b94e2995188255c5c45d0ec来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
  </entry>
  <entry>
    <title><![CDATA[BAT 经典算法笔试题 —— 磁盘多路归并排序]]></title>
    <url>%2F2019%2F01%2F22%2Fyuque%2FBAT%20%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E7%AC%94%E8%AF%95%E9%A2%98%20%E2%80%94%E2%80%94%20%E7%A3%81%E7%9B%98%E5%A4%9A%E8%B7%AF%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[本文由 简悦 SimpRead 转码， 原文地址 https://juejin.im/post/5c3fe473f265da615c5980de 在 LevelDB 数据库中高层数据下沉到低层时需要经历一次 Major Compaction，将高层文件的有序键值对和低层文件的多个有序键值对进行归并排序。磁盘多路归并排序算法的输入是来自多个磁盘文件的有序键值对，在内存中将这些文件的键值对进行排序，然后输出到一到多个新的磁盘文件中。 多路归并排序在大数据领域也是常用的算法，常用于海量数据排序。当数据量特别大时，这些数据无法被单个机器内存容纳，它需要被切分位多个集合分别由不同的机器进行内存排序（map 过程），然后再进行多路归并算法将来自多个不同机器的数据进行排序（reduce 过程），这是流式多路归并排序，为什么说是流式排序呢，因为数据源来源于网络套接字。 多路归并排序的优势在于内存消耗极低，它的内存占用和输入文件的数量成正比，和数据总量无关，数据总量只会线性正比影响排序的时间。 下面我们来亲自实现一下磁盘多路归并算法，为什么是磁盘，因为它的输入来自磁盘文件。 算法思路我们需要在内存里维护一个有序数组。每个输入文件当前最小的元素作为一个元素放在数组里。数组按照元素的大小保持排序状态。 接下来我们开始进入循环，循环的逻辑总是从最小的元素下手，在其所在的文件取出下一个元素，和当前数组中的元素进行比较。根据比较结果进行不同的处理，这里我们使用二分查找算法进行快速比较。注意每个输入文件里面的元素都是有序的。 如果取出来的元素和当前数组中的最小元素相等，那么就可以直接将这个元素输出。再继续下一轮循环。不可能取出比当前数组最小元素还要小的元素，因为输入文件本身也是有序的。 否则就需要将元素插入到当前的数组中的指定位置，继续保持数组有序。然后将数组中当前最小的元素输出并移除。再进行下一轮循环。 3. 如果遇到文件结尾，那就无法继续调用 next() 方法了，这时可以直接将数组中的最小元素输出并移除，数组也跟着变小了。再进行下一轮循环。当数组空了，说明所有的文件都处理完了，算法就可以结束了。 值得注意的是，数组中永远不会存在同一个文件的两个元素，如此才保证了数组的长度不会超过输入文件的数量，同时它也不会把没有结尾的文件挤出数组导致漏排序的问题。 二分查找需要特别注意的是 Java 内置了二分查找算法在使用上比较精巧。 12345678910111213public class Collections &#123; ... public static &lt;T&gt; int binarySearch(List&lt;T&gt; list, T key) &#123; ... if (found) &#123; return index; &#125; else &#123; return -(insertIndex+1); &#125; &#125; ...&#125;复制代码 如果 key 可以在 list 中找到，那就直接返回相应的位置。如果找不到，它会返回负数，还不是简单的 -1，这个负数指明了插入的位置，也就是说在这个位置插入 key，数组将可以继续保持有序。 比如 binarySearch 返回了 index=-1，那么 insertIndex 就是 -(index+1)，也就是 0，插入点在数组开头。如果返回了 index=-size-1，那么 insertIndex 就是 size，是数组末尾。其它负数会插入数组中间。 输入文件类对于每一个输入文件都会创建一个 MergeSource 对象，它提供了 hasNext() 和 next() 方法用于判断和获取下一个元素。注意输入文件是有序的，下一个元素就是当前输入文件最小的元素。 hasNext() 方法负责读取下一行并缓存在 cachedLine 变量中，调用 next() 方法将 cachedLine 变量转换成整数并返回。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class MergeSource implements Closeable &#123; private BufferedReader reader; private String cachedLine; private String filename; public MergeSource(String filename) &#123; this.filename = filename; try &#123; FileReader fr = new FileReader(filename); this.reader = new BufferedReader(fr); &#125; catch (FileNotFoundException e) &#123; &#125; &#125; public boolean hasNext() &#123; String line; try &#123; line = this.reader.readLine(); if (line == null || line.isEmpty()) &#123; return false; &#125; this.cachedLine = line.trim(); return true; &#125; catch (IOException e) &#123; &#125; return false; &#125; public int next() &#123; if (this.cachedLine == null) &#123; if (!hasNext()) &#123; throw new IllegalStateException(&quot;no content&quot;); &#125; &#125; int num = Integer.parseInt(this.cachedLine); this.cachedLine = null; return num; &#125; @Override public void close() throws IOException &#123; this.reader.close(); &#125;&#125;复制代码 内存有序数组元素类在排序前先把这个数组准备好，将每个输入文件的最小元素放入数组，并排序。 1234567891011121314151617181920212223242526class Bin implements Comparable&lt;Bin&gt; &#123; int num; MergeSource source; Bin(MergeSource source, int num) &#123; this.source = source; this.num = num; &#125; @Override public int compareTo(Bin o) &#123; return this.num - o.num; &#125;&#125;List&lt;Bin&gt; prepare() &#123; List&lt;Bin&gt; bins = new ArrayList&lt;&gt;(); for (MergeSource source : sources) &#123; Bin newBin = new Bin(source, source.next()); bins.add(newBin); &#125; Collections.sort(bins); return bins;&#125;复制代码 输出文件类关闭输出文件时注意要先 flush()，避免丢失 PrintWriter 中缓冲的内容。 12345678910111213141516171819202122class MergeOut implements Closeable &#123; private PrintWriter writer; public MergeOut(String filename) &#123; try &#123; FileOutputStream out = new FileOutputStream(filename); this.writer = new PrintWriter(out); &#125; catch (FileNotFoundException e) &#123; &#125; &#125; public void write(Bin bin) &#123; writer.println(bin.num); &#125; @Override public void close() throws IOException &#123; writer.flush(); writer.close(); &#125;&#125;复制代码 准备输入文件的内容下面我们来生成一系列输入文件，每个输入文件中包含一堆随机整数。一共生成 n 个文件，每个文件的整数数量在 minEntries 到 minEntries 之间。返回所有输入文件的文件名列表。 123456789101112131415161718192021222324252627List&lt;String&gt; generateFiles(int n, int minEntries, int maxEntries) &#123; List&lt;String&gt; files = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; n; i++) &#123; String filename = &quot;input-&quot; + i + &quot;.txt&quot;; PrintWriter writer; try &#123; writer = new PrintWriter(new FileOutputStream(filename)); ThreadLocalRandom rand = ThreadLocalRandom.current(); int entries = rand.nextInt(minEntries, maxEntries); List&lt;Integer&gt; nums = new ArrayList&lt;&gt;(); for (int k = 0; k &lt; entries; k++) &#123; int num = rand.nextInt(10000000); nums.add(num); &#125; Collections.sort(nums); for (int num : nums) &#123; writer.println(num); &#125; writer.flush(); writer.close(); &#125; catch (FileNotFoundException e) &#123; &#125; files.add(filename); &#125; return files;&#125;复制代码 排序算法万事俱备，只欠东风。将上面的类都准备好之后，排序算法很简单，代码量非常少。对照上面算法思路来理解下面的算法就很容易了。 123456789101112131415161718192021222324252627282930313233public void sort() &#123; List&lt;Bin&gt; bins = prepare(); while (true) &#123; // 取数组中最小的元素 MergeSource current = bins.get(0).source; if (current.hasNext()) &#123; // 从输入文件中取出下一个元素 Bin newBin = new Bin(current, current.next()); // 二分查找，也就是和数组中已有元素进行比较 int index = Collections.binarySearch(bins, newBin); if (index == 0) &#123; // 算法思路情况1 this.out.write(newBin); &#125; else &#123; // 算法思路情况2 if (index &lt; 0) &#123; index = -(index+1）; &#125; bins.add(index, newBin); Bin minBin = bins.remove(0); this.out.write(minBin); &#125; &#125; else &#123; // 算法思路情况3:遇到文件尾 Bin minBin = bins.remove(0); this.out.write(minBin); if (bins.isEmpty()) &#123; break; &#125; &#125; &#125;&#125;复制代码 全部代码读者可以直接将下面的代码拷贝粘贴到 IDE 中运行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193package leetcode;import java.io.BufferedReader;import java.io.Closeable;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.FileReader;import java.io.IOException;import java.io.PrintWriter;import java.util.ArrayList;import java.util.Collections;import java.util.List;import java.util.concurrent.ThreadLocalRandom;public class DiskMergeSort implements Closeable &#123; public static List&lt;String&gt; generateFiles(int n, int minEntries, int maxEntries) &#123; List&lt;String&gt; files = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; n; i++) &#123; String filename = &quot;input-&quot; + i + &quot;.txt&quot;; PrintWriter writer; try &#123; writer = new PrintWriter(new FileOutputStream(filename)); int entries = ThreadLocalRandom.current().nextInt(minEntries, maxEntries); List&lt;Integer&gt; nums = new ArrayList&lt;&gt;(); for (int k = 0; k &lt; entries; k++) &#123; int num = ThreadLocalRandom.current().nextInt(10000000); nums.add(num); &#125; Collections.sort(nums); for (int num : nums) &#123; writer.println(num); &#125; writer.close(); &#125; catch (FileNotFoundException e) &#123; &#125; files.add(filename); &#125; return files; &#125; private List&lt;MergeSource&gt; sources; private MergeOut out; public DiskMergeSort(List&lt;String&gt; files, String outFilename) &#123; this.sources = new ArrayList&lt;&gt;(); for (String filename : files) &#123; this.sources.add(new MergeSource(filename)); &#125; this.out = new MergeOut(outFilename); &#125; static class MergeOut implements Closeable &#123; private PrintWriter writer; public MergeOut(String filename) &#123; try &#123; this.writer = new PrintWriter(new FileOutputStream(filename)); &#125; catch (FileNotFoundException e) &#123; &#125; &#125; public void write(Bin bin) &#123; writer.println(bin.num); &#125; @Override public void close() throws IOException &#123; writer.flush(); writer.close(); &#125; &#125; static class MergeSource implements Closeable &#123; private BufferedReader reader; private String cachedLine; public MergeSource(String filename) &#123; try &#123; FileReader fr = new FileReader(filename); this.reader = new BufferedReader(fr); &#125; catch (FileNotFoundException e) &#123; &#125; &#125; public boolean hasNext() &#123; String line; try &#123; line = this.reader.readLine(); if (line == null || line.isEmpty()) &#123; return false; &#125; this.cachedLine = line.trim(); return true; &#125; catch (IOException e) &#123; &#125; return false; &#125; public int next() &#123; if (this.cachedLine == null) &#123; if (!hasNext()) &#123; throw new IllegalStateException(&quot;no content&quot;); &#125; &#125; int num = Integer.parseInt(this.cachedLine); this.cachedLine = null; return num; &#125; @Override public void close() throws IOException &#123; this.reader.close(); &#125; &#125; static class Bin implements Comparable&lt;Bin&gt; &#123; int num; MergeSource source; Bin(MergeSource source, int num) &#123; this.source = source; this.num = num; &#125; @Override public int compareTo(Bin o) &#123; return this.num - o.num; &#125; &#125; public List&lt;Bin&gt; prepare() &#123; List&lt;Bin&gt; bins = new ArrayList&lt;&gt;(); for (MergeSource source : sources) &#123; Bin newBin = new Bin(source, source.next()); bins.add(newBin); &#125; Collections.sort(bins); return bins; &#125; public void sort() &#123; List&lt;Bin&gt; bins = prepare(); while (true) &#123; MergeSource current = bins.get(0).source; if (current.hasNext()) &#123; Bin newBin = new Bin(current, current.next()); int index = Collections.binarySearch(bins, newBin); if (index == 0 || index == -1) &#123; this.out.write(newBin); if (index == -1) &#123; throw new IllegalStateException(&quot;impossible&quot;); &#125; &#125; else &#123; if (index &lt; 0) &#123; index = -index - 1; &#125; bins.add(index, newBin); Bin minBin = bins.remove(0); this.out.write(minBin); &#125; &#125; else &#123; Bin minBin = bins.remove(0); this.out.write(minBin); if (bins.isEmpty()) &#123; break; &#125; &#125; &#125; &#125; @Override public void close() throws IOException &#123; for (MergeSource source : sources) &#123; source.close(); &#125; this.out.close(); &#125; public static void main(String[] args) throws IOException &#123; List&lt;String&gt; inputs = DiskMergeSort.generateFiles(100, 10000, 20000); // 运行多次看算法耗时 for (int i = 0; i &lt; 20; i++) &#123; DiskMergeSort sorter = new DiskMergeSort(inputs, &quot;output.txt&quot;); long start = System.currentTimeMillis(); sorter.sort(); long duration = System.currentTimeMillis() - start; System.out.printf(&quot;%dms\n&quot;, duration); sorter.close(); &#125; &#125;&#125;复制代码 本算法还有一个小缺陷，那就是如果输入文件数量非常多，那么内存中的数组就会特别大，对数组的插入删除操作肯定会很耗时，这时可以考虑使用 TreeSet 来代替数组，读者们可以自行尝试一下。]]></content>
  </entry>
  <entry>
    <title><![CDATA[跟着动画来学习 TCP 三次握手和四次挥手]]></title>
    <url>%2F2019%2F01%2F22%2Fyuque%2F%E8%B7%9F%E7%9D%80%E5%8A%A8%E7%94%BB%E6%9D%A5%E5%AD%A6%E4%B9%A0%20TCP%20%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[本文由 简悦 SimpRead 转码， 原文地址 https://juejin.im/post/5b29d2c4e51d4558b80b1d8c TCP 三次握手和四次挥手的问题在面试中是最为常见的考点之一。很多读者都知道三次和四次，但是如果问深入一点，他们往往都无法作出准确回答。 本篇尝试使用动画来对这个知识点进行讲解，期望读者们可以更加简单地地理解 TCP 交互的本质。 TCP 三次握手TCP 三次握手就好比两个人在街上隔着 50 米看见了对方，但是因为雾霾等原因不能 100% 确认，所以要通过招手的方式相互确定对方是否认识自己。 张三首先向李四招手 (syn)，李四看到张三向自己招手后，向对方点了点头挤出了一个微笑 (ack)。张三看到李四微笑后确认了李四成功辨认出了自己 (进入 estalished 状态)。 但是李四还有点狐疑，向四周看了一看，有没有可能张三是在看别人呢，他也需要确认一下。所以李四也向张三招了招手 (syn)，张三看到李四向自己招手后知道对方是在寻求自己的确认，于是也点了点头挤出了微笑 (ack)，李四看到对方的微笑后确认了张三就是在向自己打招呼 (进入 established 状态)。 于是两人加快步伐，走到了一起，相互拥抱。 我们看到这个过程中一共是四个动作，张三招手 – 李四点头微笑 – 李四招手 – 张三点头微笑。其中李四连续进行了 2 个动作，先是点头微笑 (回复对方)，然后再次招手 (寻求确认)，实际上可以将这两个动作合一，招手的同时点头和微笑 (syn+ack)。于是四个动作就简化成了三个动作，张三招手 – 李四点头微笑并招手 – 张三点头微笑。这就是三次握手的本质，中间的一次动作是两个动作的合并。 我们看到有两个中间状态，syn_sent 和 syn_rcvd，这两个状态叫着「半打开」状态，就是向对方招手了，但是还没来得及看到对方的点头微笑。syn_sent 是主动打开方的「半打开」状态，syn_rcvd 是被动打开方的「半打开」状态。客户端是主动打开方，服务器是被动打开方。 syn_sent: syn package has been sent syn_rcvd: syn package has been received TCP 数据传输TCP 数据传输就是两个人隔空对话，差了一点距离，所以需要对方反复确认听见了自己的话。 张三喊了一句话 (data)，李四听见了之后要向张三回复自己听见了 (ack)。 如果张三喊了一句，半天没听到李四回复，张三就认为自己的话被大风吹走了，李四没听见，所以需要重新喊话，这就是 tcp 重传。 也有可能是李四听到了张三的话，但是李四向张三的回复被大风吹走了，以至于张三没听见李四的回复。张三并不能判断究竟是自己的话被大风吹走了还是李四的回复被大风吹走了，张三也不用管，重传一下就是。 既然会重传，李四就有可能同一句话听见了两次，这就是「去重」。「重传」和「去重」工作操作系统的网络内核模块都已经帮我们处理好了，用户层是不用关心的。 张三可以向李四喊话，同样李四也可以向张三喊话，因为 tcp 链接是「双工的」，双方都可以主动发起数据传输。不过无论是哪方喊话，都需要收到对方的确认才能认为对方收到了自己的喊话。 张三可能是个高射炮，一说连说了八句话，这时候李四可以不用一句一句回复，而是连续听了这八句话之后，一起向对方回复说前面你说的八句话我都听见了，这就是批量 ack。但是张三也不能一次性说了太多话，李四的脑子短时间可能无法消化太多，两人之间需要有协商好的合适的发送和接受速率，这个就是「TCP 窗口大小」。 网络环境的数据交互同人类之间的对话还要复杂一些，它存在数据包乱序的现象。同一个来源发出来的不同数据包在「网际路由」上可能会走过不同的路径，最终达到同一个地方时，顺序就不一样了。操作系统的网络内核模块会负责对数据包进行排序，到用户层时顺序就已经完全一致了。 TCP 四次挥手TCP 断开链接的过程和建立链接的过程比较类似，只不过中间的两部并不总是会合成一步走，所以它分成了 4 个动作，张三挥手 (fin)——李四伤感地微笑 (ack)——李四挥手 (fin)——张三伤感地微笑 (ack)。 之所以中间的两个动作没有合并，是因为 tcp 存在「半关闭」状态，也就是单向关闭。张三已经挥了手，可是人还没有走，只是不再说话，但是耳朵还是可以继续听，李四呢继续喊话。等待李四累了，也不再说话了，朝张三挥了挥手，张三伤感地微笑了一下，才彻底结束了。 上面有一个非常特殊的状态time_wait，它是主动关闭的一方在回复完对方的挥手后进入的一个长期状态，这个状态标准的持续时间是 4 分钟，4 分钟后才会进入到 closed 状态，释放套接字资源。不过在具体实现上这个时间是可以调整的。 它就好比主动分手方要承担的责任，是你提出的要分手，你得付出代价。这个后果就是持续 4 分钟的time_wait状态，不能释放套接字资源 (端口)，就好比守寡期，这段时间内套接字资源(端口) 不得回收利用。 它的作用是重传最后一个 ack 报文，确保对方可以收到。因为如果对方没有收到 ack 的话，会重传 fin 报文，处于 time_wait 状态的套接字会立即向对方重发 ack 报文。 同时在这段时间内，该链接在对话期间于网际路由上产生的残留报文 (因为路径过于崎岖，数据报文走的时间太长，重传的报文都收到了，原始报文还在路上) 传过来时，都会被立即丢弃掉。4 分钟的时间足以使得这些残留报文彻底消逝。不然当新的端口被重复利用时，这些残留报文可能会干扰新的链接。 4 分钟就是 2 个 MSL，每个 MSL 是 2 分钟。MSL 就是maximium segment lifetime——最长报文寿命。这个时间是由官方 RFC 协议规定的。至于为什么是 2 个 MSL 而不是 1 个 MSL，我还没有看到一个非常满意的解释。 四次挥手也并不总是四次挥手，中间的两个动作有时候是可以合并一起进行的，这个时候就成了三次挥手，主动关闭方就会从fin_wait_1状态直接进入到time_wait状态，跳过了fin_wait_2状态。 总结TCP 状态转换是一个非常复杂的过程，本文仅对一些简单的基础知识点进行了类比讲解。关于 TCP 的更多知识还需要读者去搜寻相关技术文章进入深入学习。如果读者对 TCP 的基础知识掌握得比较牢固，高级的知识理解起来就不会太过于吃力。 关于 TCP 的更多文章，还请关注微信公众号「码洞」进行订阅，后续我会持续更新更多细节。 如果觉得本文写的质量还可以，就给我的掘金小册捧捧场吧 深入理解 RPC 深入理解 RPC 深入理解 RPC 深入理解 RPC 深入理解 RPC 深入理解 RPC]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java8 新特性指导手册]]></title>
    <url>%2F2019%2F01%2F22%2Fyuque%2FJava8%20%E6%96%B0%E7%89%B9%E6%80%A7%E6%8C%87%E5%AF%BC%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[本文由 简悦 SimpRead 转码， 原文地址 https://juejin.im/post/5c3d7c8a51882525dd591ac7 本教程翻译整理自 github.com/winterbe/ja… 目录： 一、接口内允许添加默认实现的方法 二、Lambda 表达式 三、函数式接口 Functional Interface 四、便捷的引用类的构造器及方法 五、Lambda 访问外部变量及接口默认方法 5.1 访问局部变量 5.2 访问成员变量和静态变量 5.3 访问接口的默认方法 六、内置的函数式接口 6.1 Predicate 断言 6.2 Function 6.3 Supplier 生产者 6.4 Consumer 消费者 6.5 Comparator 七、Optional 八、Streams 流 8.1 Filter 过滤 8.2 Sorted 排序 8.3 Map 转换 8.4 Match 匹配 8.5 Count 计数 8.6 Reduce 九、Parallel Streams 并行流 9.1 顺序流排序 9.2 并行流排序 十、Map 集合 十一、新的日期 API 11.1 Clock 11.2 Timezones 时区 11.3 LocalTime 11.4 LocalDate 11.4 LocalDateTime 十二、Annotations 注解 也希望学完本系列教程的小伙伴能够熟练掌握和应用 Java8 的各种特性，使其成为在工作中的一门利器。废话不多说，让我们一起开启 Java8 新特性之旅吧！ ★★★ 如果此教程有帮助到你, 去小哈的 GitHub 帮忙 **Star 一下吧, 谢谢啦！** 传送门 ★★★ 接口内允许添加默认实现的方法Java 8 允许我们通过 default 关键字对接口中定义的抽象方法提供一个默认的实现。 请看下面示例代码： 1234567891011// 定义一个公式接口interface Formula &#123; // 计算 double calculate(int a); // 求平方根 default double sqrt(int a) &#123; return Math.sqrt(a); &#125;&#125;复制代码 在上面这个接口中，我们除了定义了一个抽象方法 calculate，还定义了一个带有默认实现的方法 sqrt。 我们在实现这个接口时，可以只需要实现 calculate 方法，默认方法 sqrt 可以直接调用即可，也就是说我们可以不必强制实现 sqrt 方法。 补充：通过 default 关键字这个新特性，可以非常方便地对之前的接口做拓展，而此接口的实现类不必做任何改动。 12345678910Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100); &#125;&#125;;formula.calculate(100); // 100.0formula.sqrt(16); // 4.0复制代码 上面通过匿名对象实现了 Formula 接口。但是即使是这样，我们为了完成一个 sqrt(a * 100) 简单计算，就写了 6 行代码，很是冗余。 Lambda 表达式在学习 Lambda 表达式之前，我们先来看一段老版本的示例代码，其对一个含有字符串的集合进行排序： 123456789List&lt;String&gt; names = Arrays.asList(&quot;peter&quot;, &quot;anna&quot;, &quot;mike&quot;, &quot;xenia&quot;);Collections.sort(names, new Comparator&lt;String&gt;() &#123; @Override public int compare(String a, String b) &#123; return b.compareTo(a); &#125;&#125;);复制代码 Collections 工具类提供了静态方法 sort 方法，入参是一个 List 集合，和一个 Comparator 比较器，以便对给定的 List 集合进行 排序。上面的示例代码创建了一个匿名内部类作为入参，这种类似的操作在我们日常的工作中随处可见。 Java 8 中不再推荐这种写法，而是推荐使用 Lambda 表达： 1234Collections.sort(names, (String a, String b) -&gt; &#123; return b.compareTo(a);&#125;);复制代码 正如你看到的，上面这段代码变得简短很多而且易于阅读。但是我们还可以再精炼一点： 12Collections.sort(names, (String a, String b) -&gt; b.compareTo(a));复制代码 对于只包含一行方法的代码块，我们可以省略大括号，直接 return 关键代码即可。追求极致，我们还可以让它再短点： 12names.sort((a, b) -&gt; b.compareTo(a));复制代码 List 集合现在已经添加了 sort 方法。而且 Java 编译器能够根据类型推断机制判断出参数类型，这样，你连入参的类型都可以省略啦，怎么样，是不是感觉很强大呢！ 函数式接口 Functional Interface抛出一个疑问：在我们书写一段 Lambda 表达式后（比如上一章节中匿名内部类的 Lambda 表达式缩写形式），Java 编译器是如何进行类型推断的，它又是怎么知道重写的哪个方法的？ 需要说明的是，不是每个接口都可以缩写成 Lambda 表达式。只有那些函数式接口（Functional Interface）才能缩写成 Lambda 表示式。 那么什么是函数式接口（Functional Interface）呢？ 所谓函数式接口（Functional Interface）就是只包含一个抽象方法的声明。针对该接口类型的所有 Lambda 表达式都会与这个抽象方法匹配。 注意：你可能会有疑问，Java 8 中不是允许通过 defualt 关键字来为接口添加默认方法吗？那它算不算抽象方法呢？答案是：不算。因此，你可以毫无顾忌的添加默认方法，它并不违反函数式接口（Functional Interface）的定义。 总结一下：只要接口中仅仅包含一个抽象方法，我们就可以将其改写为 Lambda 表达式。为了保证一个接口明确的被定义为一个函数式接口（Functional Interface），我们需要为该接口添加注解：@FunctionalInterface。这样，一旦你添加了第二个抽象方法，编译器会立刻抛出错误提示。 示例代码： 12345@FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125;复制代码 示例代码 2： 1234Converter&lt;String, Integer&gt; converter = (from) -&gt; Integer.valueOf(from);Integer converted = converter.convert(&quot;123&quot;);System.out.println(converted); // 123复制代码 注意：上面的示例代码，即使去掉 @FunctionalInterface 也是好使的，它仅仅是一种约束而已。 便捷的引用类的构造器及方法小伙伴们，还记得上一个章节这段示例代码么： 12345@FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125;复制代码 1234Converter&lt;String, Integer&gt; converter = (from) -&gt; Integer.valueOf(from);Integer converted = converter.convert(&quot;123&quot;);System.out.println(converted); // 123复制代码 上面这段代码，通过 Java 8 的新特性，进一步简化上面的代码： 1234Converter&lt;String, Integer&gt; converter = Integer::valueOf;Integer converted = converter.convert(&quot;123&quot;);System.out.println(converted); // 123复制代码 Java 8 中允许你通过 :: 关键字来引用类的方法或构造器。上面的代码简单的示例了如何引用静态方法，当然，除了静态方法，我们还可以引用普通方法： 123456class Something &#123; String startsWith(String s) &#123; return String.valueOf(s.charAt(0)); &#125;&#125;复制代码 12345Something something = new Something();Converter&lt;String, String&gt; converter = something::startsWith;String converted = converter.convert(&quot;Java&quot;);System.out.println(converted); // &quot;J&quot;复制代码 接下来，我们再来看看如何通过 :: 关键字来引用类的构造器。首先，我们先来定义一个示例类，在类中声明两个构造器： 123456789101112class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125;复制代码 然后，我们再定义一个工厂接口，用来生成 Person 类： 12345// Person 工厂interface PersonFactory&lt;P extends Person&gt; &#123; P create(String firstName, String lastName);&#125;复制代码 我们可以通过 :: 关键字来引用 Person 类的构造器，来代替手动去实现这个工厂接口： 1234// 直接引用 Person 构造器PersonFactory&lt;Person&gt; personFactory = Person::new;Person person = personFactory.create(&quot;Peter&quot;, &quot;Parker&quot;);复制代码 Person::new 这段代码，能够直接引用 Person 类的构造器。然后 Java 编译器能够根据上下文选中正确的构造器去实现 PersonFactory.create 方法。 Lambda 访问外部变量及接口默认方法在本章节中，我们将会讨论如何在 lambda 表达式中访问外部变量（包括：局部变量，成员变量，静态变量，接口的默认方法.），它与匿名内部类访问外部变量很相似。 访问局部变量在 Lambda 表达式中，我们可以访问外部的 final 类型变量，如下面的示例代码： 123456// 转换器@FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125;复制代码 123456final int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3复制代码 与匿名内部类不同的是，我们不必显式声明 num 变量为 final 类型，下面这段代码同样有效： 123456int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3复制代码 但是 num 变量必须为隐式的 final 类型，何为隐式的 final 呢？就是说到编译期为止，num 对象是不能被改变的，如下面这段代码，就不能被编译通过： 12345int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);num = 3;复制代码 在 lambda 表达式内部改变 num 值同样编译不通过，需要注意, 比如下面的示例代码： 1234567int num = 1;Converter&lt;Integer, String&gt; converter = (from) -&gt; &#123; String value = String.valueOf(from + num); num = 3; return value;&#125;;复制代码 访问成员变量和静态变量上一章节中，了解了如何在 Lambda 表达式中访问局部变量。与局部变量相比，在 Lambda 表达式中对成员变量和静态变量拥有读写权限： 12345@FunctionalInterface interface Converter&lt;F, T&gt; &#123; T convert(F from); &#125;复制代码 123456789101112131415161718192021class Lambda4 &#123; // 静态变量 static int outerStaticNum; // 成员变量 int outerNum; void testScopes() &#123; Converter&lt;Integer, String&gt; stringConverter1 = (from) -&gt; &#123; // 对成员变量赋值 outerNum = 23; return String.valueOf(from); &#125;; Converter&lt;Integer, String&gt; stringConverter2 = (from) -&gt; &#123; // 对静态变量赋值 outerStaticNum = 72; return String.valueOf(from); &#125;; &#125; &#125;复制代码 访问接口的默认方法还记得第一章节中定义的那个 Formula (公式) 接口吗？ 1234567891011@FunctionalInterfaceinterface Formula &#123; // 计算 double calculate(int a); // 求平方根 default double sqrt(int a) &#123; return Math.sqrt(a); &#125;&#125;复制代码 当时，我们在接口中定义了一个带有默认实现的 sqrt 求平方根方法，在匿名内部类中我们可以很方便的访问此方法： 1234567Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100); &#125;&#125;;复制代码 但是在 lambda 表达式中可不行： 12Formula formula = (a) -&gt; sqrt(a * 100);复制代码 带有默认实现的接口方法，是不能在 lambda 表达式中访问的，上面这段代码将无法被编译通过。 内置的函数式接口JDK 1.8 API 包含了很多内置的函数式接口。其中就包括我们在老版本中经常见到的 Comparator 和 Runnable，Java 8 为他们都添加了 FunctionalInterface 注解，以用来支持 Lambda 表达式。 值得一提的是，除了 Comparator 和 Runnable 外，还有一些新的函数式接口，它们很多都借鉴于知名的 Google Guava 库。 对于它们，即使你已经非常熟悉了，还是最好了解一下的： Predicate 断言Predicate 是一个可以指定入参类型，并返回 boolean 值的函数式接口。它内部提供了一些带有默认实现的方法，可以 被用来组合一个复杂的逻辑判断（and, or, negate）： 1234567891011Predicate&lt;String&gt; predicate = (s) -&gt; s.length() &gt; 0;predicate.test(&quot;foo&quot;); // truepredicate.negate().test(&quot;foo&quot;); // falsePredicate&lt;Boolean&gt; nonNull = Objects::nonNull;Predicate&lt;Boolean&gt; isNull = Objects::isNull;Predicate&lt;String&gt; isEmpty = String::isEmpty;Predicate&lt;String&gt; isNotEmpty = isEmpty.negate();复制代码 FunctionFunction 函数式接口的作用是，我们可以为其提供一个原料，他给生产一个最终的产品。通过它提供的默认方法，组合, 链行处理 (compose, andThen)： 12345Function&lt;String, Integer&gt; toInteger = Integer::valueOf;Function&lt;String, String&gt; backToString = toInteger.andThen(String::valueOf);backToString.apply(&quot;123&quot;); // &quot;123&quot;复制代码 Supplier 生产者Supplier 与 Function 不同，它不接受入参，直接为我们生产一个指定的结果，有点像生产者模式： 123456789101112class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125;复制代码 123Supplier&lt;Person&gt; personSupplier = Person::new;personSupplier.get(); // new Person复制代码 Consumer 消费者对于 Consumer，我们需要提供入参，用来被消费，如下面这段示例代码： 123456789101112class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125;复制代码 123Consumer&lt;Person&gt; greeter = (p) -&gt; System.out.println(&quot;Hello, &quot; + p.firstName);greeter.accept(new Person(&quot;Luke&quot;, &quot;Skywalker&quot;));复制代码 ComparatorComparator 在 Java 8 之前是使用比较普遍的。Java 8 中除了将其升级成了函数式接口，还为它拓展了一些默认方法： 12345678Comparator&lt;Person&gt; comparator = (p1, p2) -&gt; p1.firstName.compareTo(p2.firstName);Person p1 = new Person(&quot;John&quot;, &quot;Doe&quot;);Person p2 = new Person(&quot;Alice&quot;, &quot;Wonderland&quot;);comparator.compare(p1, p2); // &gt; 0comparator.reversed().compare(p1, p2); // &lt; 0复制代码 Optional首先，Optional 它不是一个函数式接口，设计它的目的是为了防止空指针异常（NullPointerException），要知道在 Java 编程中， 空指针异常可是臭名昭著的。 让我们来快速了解一下 Optional 要如何使用！你可以将 Optional 看做是包装对象（可能是 null, 也有可能非 null）的容器。当你定义了 一个方法，这个方法返回的对象可能是空，也有可能非空的时候，你就可以考虑用 Optional 来包装它，这也是在 Java 8 被推荐使用的做法。 12345678Optional&lt;String&gt; optional = Optional.of(&quot;bam&quot;);optional.isPresent(); // trueoptional.get(); // &quot;bam&quot;optional.orElse(&quot;fallback&quot;); // &quot;bam&quot;optional.ifPresent((s) -&gt; System.out.println(s.charAt(0))); // &quot;b&quot;复制代码 Stream 流这一章节，我们开始步入学习 Stream 流。 什么是 Stream 流？ 简单来说，我们可以使用 java.util.Stream 对一个包含一个或多个元素的集合做各种操作。这些操作可能是 中间操作 亦或是 _终端操作_。 终端操作会返回一个结果，而中间操作会返回一个 Stream 流。 需要注意的是，你只能对实现了 java.util.Collection 接口的类做流的操作。 Map 不支持 Stream 流。 Stream 流支持同步执行，也支持并发执行。 让我们开始步入学习的旅程吧！Go ! Filter 过滤首先，我们创建一个 List 集合： 12345678910List&lt;String&gt; stringCollection = new ArrayList&lt;&gt;();stringCollection.add(&quot;ddd2&quot;);stringCollection.add(&quot;aaa2&quot;);stringCollection.add(&quot;bbb1&quot;);stringCollection.add(&quot;aaa1&quot;);stringCollection.add(&quot;bbb3&quot;);stringCollection.add(&quot;ccc&quot;);stringCollection.add(&quot;bbb2&quot;);stringCollection.add(&quot;ddd1&quot;);复制代码 Filter 的入参是一个 Predicate, 上面已经说到，Predicate 是一个断言的中间操作，它能够帮我们筛选出我们需要的集合元素。它的返参同样 是一个 Stream 流，我们可以通过 foreach 终端操作，来打印被筛选的元素： 1234567stringCollection .stream() .filter((s) -&gt; s.startsWith(&quot;a&quot;)) .forEach(System.out::println);// &quot;aaa2&quot;, &quot;aaa1&quot;复制代码 注意：foreach 是一个终端操作，它的返参是 void, 我们无法对其再次进行流操作。 Sorted 排序Sorted 同样是一个中间操作，它的返参是一个 Stream 流。另外，我们可以传入一个 Comparator 用来自定义排序，如果不传，则使用默认的排序规则。 12345678stringCollection .stream() .sorted() .filter((s) -&gt; s.startsWith(&quot;a&quot;)) .forEach(System.out::println);// &quot;aaa1&quot;, &quot;aaa2&quot;复制代码 需要注意，sorted 不会对 stringCollection 做出任何改变，stringCollection 还是原有的那些个元素，且顺序不变： 123System.out.println(stringCollection);// ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1复制代码 Map 转换中间操作 Map 能够帮助我们将 List 中的每一个元素做功能处理。例如下面的示例，通过 map 我们将每一个 string 转成大写： 12345678stringCollection .stream() .map(String::toUpperCase) .sorted((a, b) -&gt; b.compareTo(a)) .forEach(System.out::println);// &quot;DDD2&quot;, &quot;DDD1&quot;, &quot;CCC&quot;, &quot;BBB3&quot;, &quot;BBB2&quot;, &quot;AAA2&quot;, &quot;AAA1&quot;复制代码 另外，我们还可以做对象之间的转换，业务中比较常用的是将 DO（数据库对象） 转换成 BO（业务对象） 。 Match 匹配顾名思义，match 用来做匹配操作，它的返回值是一个 boolean 类型。通过 match, 我们可以方便的验证一个 list 中是否存在某个类型的元素。 123456789101112131415161718192021222324// 验证 list 中 string 是否有以 a 开头的, 匹配到第一个，即返回 trueboolean anyStartsWithA = stringCollection .stream() .anyMatch((s) -&gt; s.startsWith(&quot;a&quot;));System.out.println(anyStartsWithA); // true// 验证 list 中 string 是否都是以 a 开头的boolean allStartsWithA = stringCollection .stream() .allMatch((s) -&gt; s.startsWith(&quot;a&quot;));System.out.println(allStartsWithA); // false// 验证 list 中 string 是否都不是以 z 开头的,boolean noneStartsWithZ = stringCollection .stream() .noneMatch((s) -&gt; s.startsWith(&quot;z&quot;));System.out.println(noneStartsWithZ); // true复制代码 Count 计数count 是一个终端操作，它能够统计 stream 流中的元素总数，返回值是 long 类型。 123456789// 先对 list 中字符串开头为 b 进行过滤，让后统计数量long startsWithB = stringCollection .stream() .filter((s) -&gt; s.startsWith(&quot;b&quot;)) .count();System.out.println(startsWithB); // 3复制代码 ReduceReduce 中文翻译为：_减少、缩小_。通过入参的 Function，我们能够将 list 归约成一个值。它的返回类型是 Optional 类型。 123456789Optional&lt;String&gt; reduced = stringCollection .stream() .sorted() .reduce((s1, s2) -&gt; s1 + &quot;#&quot; + s2);reduced.ifPresent(System.out::println);// &quot;aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2&quot;复制代码 Parallel-Streams 并行流前面章节我们说过，stream 流是支持顺序和并行的。顺序流操作是单线程操作，而并行流是通过多线程来处理的，能够充分利用物理机 多核 CPU 的优势，同时处理速度更快。 首先，我们创建一个包含 1000000 UUID list 集合。 1234567int max = 1000000;List&lt;String&gt; values = new ArrayList&lt;&gt;(max);for (int i = 0; i &lt; max; i++) &#123; UUID uuid = UUID.randomUUID(); values.add(uuid.toString());&#125;复制代码 分别通过顺序流和并行流，对这个 list 进行排序，测算耗时: 顺序流排序123456789101112131415// 纳秒long t0 = System.nanoTime();long count = values.stream().sorted().count();System.out.println(count);long t1 = System.nanoTime();// 纳秒转微秒long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format(&quot;顺序流排序耗时: %d ms&quot;, millis));// 顺序流排序耗时: 899 ms复制代码 并行流排序1234567891011121314// 纳秒long t0 = System.nanoTime();long count = values.parallelStream().sorted().count();System.out.println(count);long t1 = System.nanoTime();// 纳秒转微秒long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format(&quot;并行流排序耗时: %d ms&quot;, millis));// 并行流排序耗时: 472 ms复制代码 正如你所见，同样的逻辑处理，通过并行流，我们的性能提升了近 50%。完成这一切，我们需要做的仅仅是将 stream 改成了 parallelStream。 Map 集合前面已经提到过 Map 是不支持 Stream 流的，因为 Map 接口并没有像 Collection 接口那样，定义了 stream() 方法。但是，我们可以对其 key, values, entry 使用 流操作，如 map.keySet().stream(), map.values().stream() 和 map.entrySet().stream(). 另外, JDK 8 中对 map 提供了一些其他新特性: 1234567891011Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;();for (int i = 0; i &lt; 10; i++) &#123; // 与老版不同的是，putIfAbent() 方法在 put 之前， // 会判断 key 是否已经存在，存在则直接返回 value, 否则 put, 再返回 value map.putIfAbsent(i, &quot;val&quot; + i);&#125;// forEach 可以很方便地对 map 进行遍历操作map.forEach((key, value) -&gt; System.out.println(value));复制代码 除了上面的 putIfAbsent() 和 forEach() 外，我们还可以很方便地对某个 key 的值做相关操作： 123456789101112131415161718// computeIfPresent(), 当 key 存在时，才会做相关处理// 如下：对 key 为 3 的值，内部会先判断值是否存在，存在，则做 value + key 的拼接操作map.computeIfPresent(3, (num, val) -&gt; val + num);map.get(3); // val33// 先判断 key 为 9 的元素是否存在，存在，则做删除操作map.computeIfPresent(9, (num, val) -&gt; null);map.containsKey(9); // false// computeIfAbsent(), 当 key 不存在时，才会做相关处理// 如下：先判断 key 为 23 的元素是否存在，不存在，则添加map.computeIfAbsent(23, num -&gt; &quot;val&quot; + num);map.containsKey(23); // true// 先判断 key 为 3 的元素是否存在，存在，则不做任何处理map.computeIfAbsent(3, num -&gt; &quot;bam&quot;);map.get(3); // val33复制代码 关于删除操作，JDK 8 中提供了能够新的 remove() API: 123456map.remove(3, &quot;val3&quot;);map.get(3); // val33map.remove(3, &quot;val33&quot;);map.get(3); // null复制代码 如上代码，只有当给定的 key 和 value 完全匹配时，才会执行删除操作。 关于添加方法，JDK 8 中提供了带有默认值的 getOrDefault() 方法： 123// 若 key 42 不存在，则返回 not foundmap.getOrDefault(42, &quot;not found&quot;); // not found复制代码 对于 value 的合并操作也变得更加简单： 12345678// merge 方法，会先判断进行合并的 key 是否存在，不存在，则会添加元素map.merge(9, &quot;val9&quot;, (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9// 若 key 的元素存在，则对 value 执行拼接操作map.merge(9, &quot;concat&quot;, (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9concat复制代码 新的日期 APIJava 8 中在包 java.time 下添加了新的日期 API. 它和 Joda-Time 库相似，但又不完全相同。接下来，我会通过一些示例代码介绍一下新 API 中 最关键的特性： ClockClock 提供对当前日期和时间的访问。我们可以利用它来替代 System.currentTimeMillis() 方法。另外，通过 clock.instant() 能够获取一个 instant 实例， 此实例能够方便地转换成老版本中的 java.util.Date 对象。 123456Clock clock = Clock.systemDefaultZone();long millis = clock.millis();Instant instant = clock.instant();Date legacyDate = Date.from(instant); // 老版本 java.util.Date复制代码 Timezones 时区ZoneId 代表时区类。通过静态工厂方法方便地获取它，入参我们可以传入某个时区编码。另外，时区类还定义了一个偏移量，用来在当前时刻或某时间 与目标时区时间之间进行转换。 1234567891011System.out.println(ZoneId.getAvailableZoneIds());// prints all available timezone idsZoneId zone1 = ZoneId.of(&quot;Europe/Berlin&quot;);ZoneId zone2 = ZoneId.of(&quot;Brazil/East&quot;);System.out.println(zone1.getRules());System.out.println(zone2.getRules());// ZoneRules[currentStandardOffset=+01:00]// ZoneRules[currentStandardOffset=-03:00]复制代码 LocalTimeLocalTime 表示一个没有指定时区的时间类，例如，10 p.m. 或者 17：30:15，下面示例代码中，将会使用上面创建的 时区对象创建两个 LocalTime。然后我们会比较两个时间，并计算它们之间的小时和分钟的不同。 1234567891011LocalTime now1 = LocalTime.now(zone1);LocalTime now2 = LocalTime.now(zone2);System.out.println(now1.isBefore(now2)); // falselong hoursBetween = ChronoUnit.HOURS.between(now1, now2);long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);System.out.println(hoursBetween); // -3System.out.println(minutesBetween); // -239复制代码 LocalTime 提供多个静态工厂方法，目的是为了简化对时间对象实例的创建和操作，包括对时间字符串进行解析的操作等。 1234567891011LocalTime late = LocalTime.of(23, 59, 59);System.out.println(late); // 23:59:59DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN);LocalTime leetTime = LocalTime.parse(&quot;13:37&quot;, germanFormatter);System.out.println(leetTime); // 13:37复制代码 LocalDateLocalDate 是一个日期对象，例如：2014-03-11。它和 LocalTime 一样是个 final 类型对象。下面的例子演示了如何通过加减日，月，年等来计算一个新的日期。 LocalDate, LocalTime, 因为是 final 类型的对象，每一次操作都会返回一个新的时间对象。 1234567891011LocalDate today = LocalDate.now();// 今天加一天LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);// 明天减两天LocalDate yesterday = tomorrow.minusDays(2);// 2014 年七月的第四天LocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();System.out.println(dayOfWeek); // 星期五复制代码 也可以直接解析日期字符串，生成 LocalDate 实例。（和 LocalTime 操作一样简单） 123456789DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedDate(FormatStyle.MEDIUM) .withLocale(Locale.GERMAN);LocalDate xmas = LocalDate.parse(&quot;24.12.2014&quot;, germanFormatter);System.out.println(xmas); // 2014-12-24复制代码 LocalDateTimeLocalDateTime 是一个日期 - 时间对象。你也可以将其看成是 LocalDate 和 LocalTime 的结合体。操作上，也大致相同。 LocalDateTime 同样是一个 final 类型对象。 123456789101112LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);DayOfWeek dayOfWeek = sylvester.getDayOfWeek();System.out.println(dayOfWeek); // 星期三Month month = sylvester.getMonth();System.out.println(month); // 十二月// 获取改时间是该天中的第几分钟long minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);System.out.println(minuteOfDay); // 1439复制代码 如果再加上的时区信息，LocalDateTime 还能够被转换成 Instance 实例。Instance 能够被转换成老版本中 java.util.Date 对象。 1234567Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant();Date legacyDate = Date.from(instant);System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014复制代码 格式化 LocalDateTime 对象就和格式化 LocalDate 或者 LocalTime 一样。除了使用预定义的格式以外，也可以自定义格式化输出。 12345678DateTimeFormatter formatter = DateTimeFormatter .ofPattern(&quot;MMM dd, yyyy - HH:mm&quot;);LocalDateTime parsed = LocalDateTime.parse(&quot;Nov 03, 2014 - 07:13&quot;, formatter);String string = formatter.format(parsed);System.out.println(string); // Nov 03, 2014 - 07:13复制代码 注意：和 java.text.NumberFormat 不同，新的 DateTimeFormatter 类是 final 类型的，同时也是线程安全的。更多细节请查看这里 Annotations 注解在 Java 8 中，注解是可以重复的。让我通过下面的示例代码，来看看到底是咋回事。 首先，我们定义一个包装注解，里面包含了一个有着实际注解的数组： 123456789@interface Hints &#123; Hint[] value();&#125;@Repeatable(Hints.class)@interface Hint &#123; String value();&#125;复制代码 Java 8 中，通过 @Repeatable，允许我们对同一个类使用多重注解： 第一种形态：使用注解容器（老方法） 123@Hints(&#123;@Hint(&quot;hint1&quot;), @Hint(&quot;hint2&quot;)&#125;)class Person &#123;&#125;复制代码 第二种形态：使用可重复注解（新方法） 1234@Hint(&quot;hint1&quot;)@Hint(&quot;hint2&quot;)class Person &#123;&#125;复制代码 使用第二种形态，Java 编译器能够在内部自动对 @Hint 进行设置。这对于需要通过反射来读取注解信息时，是非常重要的。 123456789Hint hint = Person.class.getAnnotation(Hint.class);System.out.println(hint); // nullHints hints1 = Person.class.getAnnotation(Hints.class);System.out.println(hints1.value().length); // 2Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class);System.out.println(hints2.length); // 2复制代码 尽管我们绝对不会在 Person 类上声明 @Hints 注解，但是它的信息仍然是可以通过 getAnnotation(Hints.class) 来读取的。 并且，getAnnotationsByType 方法会更方便，因为它赋予了所有 @Hints 注解标注的方法直接的访问权限。 123@Target(&#123;ElementType.TYPE_PARAMETER, ElementType.TYPE_USE&#125;)@interface MyAnnotation &#123;&#125;复制代码 结语Java 8 新特性的编程指南到此就告一段落了。当然，还有很多内容需要进一步研究和说明。这就需要靠读者您来对 JDK 8 进一步探究了， 例如：Arrays.parallelSort, StampedLock 和 CompletableFuture 等等，我这里也仅是起到抛砖引玉的作用而已。 最后，我希望这个教程能够对您有所帮助，也希望您阅读愉快。 小哈的微信公众号]]></content>
  </entry>
  <entry>
    <title><![CDATA[码农的运维指南]]></title>
    <url>%2F2019%2F01%2F22%2Fyuque%2F%E7%A0%81%E5%86%9C%E7%9A%84%E8%BF%90%E7%BB%B4%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[查询时间段内的日志12sed -n '/20180905:06:20:00/,/20180905:06:30:00/p' tomcat_stdout.log-20180905sed -n '/20180905:02:10/,/20180905:02:20/p' tomcat_stdout.log-20180905 查询文件的后10行1tail -n 10 tomcat_stdout.log-20180905 查询文件的前10行1head -n 10 tomcat_stdout.log-20180905 在指定目录下查询是否有指定名称的文件1find /logs/ -name 'abc.log' 一次搜索多个gz文件中的内容1zgrep 1537354997562_xRi0 mybatis*2018-09-19.log*|grep che168_1004076]]></content>
  </entry>
</search>
